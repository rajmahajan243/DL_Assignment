{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poDfvogCn0d-"
      },
      "outputs": [],
      "source": [
        "# Name - Raj Mahajan\n",
        "# Roll Number - CS22M067\n",
        "# Assignment 1\n",
        "# Question 3\n",
        "# Optimizers combined\n",
        "# mnist dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk8K1FuZUq-N",
        "outputId": "597e205c-e61a-4048-e55b-1995928bf85d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.4.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.25.1)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.17.0-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.8)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=9244cb0856a27e7993150091de0f46869e8d48df43eeb7c95415253507f16ce1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.17.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login(key = \"c28eda24dacbd50a57332e445cfdfb2f4c16799c\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "j1QBwpKXU1_W",
        "outputId": "bcea6937-cdd8-4ee5-8568-07299c65c55f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajmahajan24\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230317_072441-d4r5hjxu</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb/runs/d4r5hjxu' target=\"_blank\">kind-music-3</a></strong> to <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb/runs/d4r5hjxu' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb/runs/d4r5hjxu</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb/runs/d4r5hjxu?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f705887d8e0>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.init(project=\"DL_A1_optimizers_combined_13.ipynb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRTp4NxgpbsO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBFE0h1Hp-k0",
        "outputId": "b79fc81a-c838-41f3-b2ca-a5d5482ffb17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(60000, 1)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the Fashion-MNIST dataset\n",
        "(X_train, Y_train), (x_test, y_test) = mnist.load_data()\n",
        "X_train=X_train.reshape(X_train.shape[0],784)\n",
        "x_test=x_test.reshape(x_test.shape[0],784)\n",
        "Y_train=Y_train.reshape(60000,1)\n",
        "y_test=y_test.reshape(10000,1)\n",
        "# X_train.shape\n",
        "Y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgZvKQQ4qDJ-",
        "outputId": "8d3fddb0-70f3-4822-f3bd-8bddd5f5ea4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 1)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Normalizing the data\n",
        "X_train=X_train/255.0\n",
        "x_test=x_test/255.0\n",
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPQgAvW_gO_w",
        "outputId": "12a7c519-2f9e-4618-fbaa-9f65742c7796"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6000, 784)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train,x_val,y_train,y_val = train_test_split(X_train , Y_train , test_size = 0.1 , random_state = 100)\n",
        "# x_train.shape\n",
        "x_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAuDaw6bo_Bu"
      },
      "outputs": [],
      "source": [
        "def actual_prob_dist(y) :\n",
        "  ans=np.zeros((10, 1))\n",
        "  ans[y]=1\n",
        "  return(ans)\n",
        "\n",
        "def sigmoid(a) :\n",
        "  # s=1/(1+np.exp(a))\n",
        "\n",
        "  if isinstance(a,np.ndarray):\n",
        "    return 1.0 / (1.0 + np.exp(-a))\n",
        "  else :\n",
        "    if (x>=0):\n",
        "      return 1.0 / (1.0 + np.exp(-a))\n",
        "    else :\n",
        "      return np.exp(a) / ( 1.0 + np.exp(a))\n",
        "  # return(s)\n",
        "\n",
        "\n",
        "\n",
        "def derivative_sigmoid(x):\n",
        "  ans = sigmoid(x) * (1 -sigmoid(x))\n",
        "  return(ans)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def derivative_relu(x):\n",
        "    return 1*(x>0)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def derivative_tanh(x):\n",
        "    return (1 - (np.tanh(x)**2))\n",
        "\n",
        "def softmax(v):\n",
        "  # ans = np.exp(v) / np.sum(np.exp(v), axis=0)\n",
        "  z = v - max(v)\n",
        "  numerator = np.exp(z)\n",
        "  denominator = np.sum(numerator)\n",
        "  ans = numerator/denominator\n",
        "  return(ans)\n",
        "\n",
        "\n",
        "def derivative_softmax(x):\n",
        "  ans= softmax(x) * (1-softmax(x))\n",
        "  return(ans)\n",
        "\n",
        "\n",
        "def cross_entropy(y, yhat, batch_size):\n",
        "   loss = (-1.0 * np.sum(np.multiply(y, np.log(yhat))))/batch_size\n",
        "   return(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQc_Udj0o_DW"
      },
      "outputs": [],
      "source": [
        "def add_list_grad(list_grad_x , list_grad_y):\n",
        "  if(len(list_grad_x) == 0 ):\n",
        "    return(list_grad_y)\n",
        "  for i in range(0,len(list_grad_x)):\n",
        "    list_grad_x[i] += list_grad_y[i]\n",
        "  return(list_grad_x)\n",
        "\n",
        "def add_list_grad_eta(list_grad_x , list_grad_y, eta):\n",
        "  if(len(list_grad_x) == 0 ):\n",
        "    return(list_grad_y)\n",
        "  for i in range(0,len(list_grad_x)):\n",
        "    list_grad_x[i] -= eta * list_grad_y[i].transpose()\n",
        "  return(list_grad_x)\n",
        "\n",
        "def initilize_theta_random_normal(num_layers,neurons):\n",
        "  theta=[]\n",
        "  i=0\n",
        "  temp=np.random.randn(neurons[0], neurons[1])\n",
        "  theta.append(temp)\n",
        "  while i< len(neurons)-3 :\n",
        "    temp=np.random.randn(neurons[i+1], neurons[i+2])\n",
        "    theta.append(temp)\n",
        "    i+=1\n",
        "  temp=np.random.randn(neurons[-2], neurons[-1])\n",
        "  theta.append(temp)\n",
        "  return(theta)\n",
        "\n",
        "def initilize_theta_random_uniform(num_layers,neurons):\n",
        "  theta=[]\n",
        "  i=0\n",
        "  temp=np.random.rand(neurons[0], neurons[1])\n",
        "  theta.append(temp)\n",
        "  while i< len(neurons)-3 :\n",
        "    temp=np.random.rand(neurons[i+1], neurons[i+2])\n",
        "    theta.append(temp)\n",
        "    i+=1\n",
        "  temp=np.random.rand(neurons[-2], neurons[-1])\n",
        "  theta.append(temp)\n",
        "  return(theta)\n",
        "\n",
        "def initilize_theta_xavier(num_layers,neurons):\n",
        "  theta=[]\n",
        "  i=0\n",
        "  temp=np.random.randn(neurons[0], neurons[1]) * np.sqrt(2/(neurons[0]  + neurons[1])) # xavier\n",
        "  theta.append(temp)\n",
        "  while i< len(neurons)-3 :\n",
        "    temp=np.random.randn(neurons[i+1], neurons[i+2]) * np.sqrt(2/(neurons[i+1] + neurons[i+2])) # xavier\n",
        "    theta.append(temp)\n",
        "    i+=1\n",
        "  temp=np.random.randn(neurons[-2], neurons[-1]) * np.sqrt(2/(neurons[-2] + neurons[-1])) # xavier\n",
        "  theta.append(temp)\n",
        "  return(theta)\n",
        "\n",
        "def initilize_bias(num_layers,neurons):\n",
        "  b=[]\n",
        "  i=0\n",
        "  while i < num_layers :\n",
        "    temp=np.zeros((1,neurons[i+1]))\n",
        "    b.append(temp)\n",
        "    i+=1\n",
        "  temp=np.zeros((1, neurons[-1]))\n",
        "  b.append(temp)\n",
        "  return(b)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJCVfydl8XrV"
      },
      "outputs": [],
      "source": [
        "def activation_fun(activation,a):\n",
        "  if(activation == \"sigmoid\"):\n",
        "    return(sigmoid(a))\n",
        "  elif(activation == \"relu\"):\n",
        "    return(relu(a))\n",
        "  elif(activation == \"tanh\"):\n",
        "    return(tanh(a))\n",
        "\n",
        "def derivative_activation_fun(activation,a):\n",
        "  if(activation == \"sigmoid\"):\n",
        "    return(derivative_sigmoid(a))\n",
        "  elif(activation == \"relu\"):\n",
        "    return(derivative_relu(a))\n",
        "  elif(activation == \"tanh\"):\n",
        "    return(derivative_tanh(a))\n",
        "  \n",
        "def loss_function(loss_fun,yhat,y):\n",
        "  if(loss_fun == \"cross_entropy\"):\n",
        "    loss = (-math.log(yhat[y]))\n",
        "  elif(loss_fun == \"mse\"):\n",
        "    Y = actual_prob_dist(y)\n",
        "    loss = (1/2) * np.sum((Y-yhat)**2)\n",
        "  return(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzS5d-JFo_E4"
      },
      "outputs": [],
      "source": [
        "# Constructed only for 1 image \n",
        "# h will be image input if 1d vector of length 28x28\n",
        "def forward_prop(activation,theta,b,num_layers,h):\n",
        "  h_list=[]\n",
        "  h_list.append(h)\n",
        "  a_list=[]\n",
        "  for k in range(0,num_layers):\n",
        "    a = b[k].transpose() + np.matmul(theta[k].transpose() , h)\n",
        "    a_list.append(a)\n",
        "    h= activation_fun(activation,a)\n",
        "    h_list.append(h)\n",
        "  # For last output layer\n",
        "  a = b[num_layers].transpose() + np.matmul(theta[num_layers].transpose() , h)\n",
        "  a_list.append(a)\n",
        "  yhat = softmax(a)\n",
        "  return(yhat,h_list,a_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7FjUsDzo_Gc"
      },
      "outputs": [],
      "source": [
        "# only for 1 image\n",
        "def backward_prop(theta,h_list,a_list,y,yhat,num_layers,batch_size,activation,loss_fun):\n",
        "  grad_a=-(actual_prob_dist(y) - yhat)\n",
        "  if(loss_fun == \"mse\"):\n",
        "      grad_a *= derivative_softmax(a_list[len(a_list)-1]);\n",
        "  list_grad_w=[]\n",
        "  list_grad_b=[]\n",
        "  for k in range(num_layers , -1,-1) :\n",
        "    grad_w = np.matmul(grad_a , h_list[k].transpose()) \n",
        "    list_grad_w.append(grad_w / batch_size)\n",
        "    grad_b = grad_a.copy()\n",
        "    list_grad_b.append(grad_b / batch_size)\n",
        "    if k >0:\n",
        "      grad_h = np.matmul(theta[k] , grad_a)\n",
        "      grad_a = grad_h * derivative_activation_fun(activation,a_list[k-1])\n",
        "  return(list_grad_w , list_grad_b)\n",
        "  \n",
        "\n",
        "# grad_b.shape = 32 x 1\n",
        "# grad_w.shape = 32 x 32\n",
        "# grad_a.shape = 10/32 x 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugPI1ENFuZIi"
      },
      "outputs": [],
      "source": [
        "def image_pridiction(x_test, y_test, theta, b, num_layers,activation):\n",
        "  img_no = 0\n",
        "  wrong_count = 0\n",
        "  while(img_no<len(x_test)):\n",
        "    img = x_test[img_no].reshape(784,1)\n",
        "    clas = y_test[img_no]\n",
        "    temp_ans = forward_prop(activation,theta,b,num_layers,img)\n",
        "    yhat = temp_ans[0]\n",
        "    temp = -1\n",
        "    predicted_class = -1\n",
        "    # print(yhat.shape)\n",
        "    sum=0\n",
        "    for i in range(0 , 10):\n",
        "      sum+=yhat[i]\n",
        "      if(temp < yhat[i]):\n",
        "        predicted_class = i\n",
        "        temp = yhat[i]\n",
        "    # print(sum)\n",
        "    # print(predicted_class)\n",
        "    if(clas != predicted_class):\n",
        "      wrong_count+=1\n",
        "    img_no+=1\n",
        "  accuracy = (len(x_test) - wrong_count) * 100 / len(x_test)\n",
        "  return(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3Jyo0kzr1jR"
      },
      "outputs": [],
      "source": [
        "def SGD(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t<epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "         \n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      y_hat,h_list,a_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # training loss\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers,batch_size,activation,loss_fun)\n",
        "      list_grad_w = add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        theta = add_list_grad_eta(theta.copy() , list_grad_w, eta)\n",
        "        b = add_list_grad_eta(b.copy(), list_grad_b , eta)\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      # print(theta)\n",
        "      image_no += 1\n",
        "\n",
        "    # Update theta for last few images\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      theta = add_list_grad_eta(theta.copy() , list_grad_w, eta)\n",
        "      b = add_list_grad_eta(b.copy(), list_grad_b , eta)\n",
        "\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "\n",
        "  return(theta,b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyITICU2r1UN"
      },
      "outputs": [],
      "source": [
        "def momentum_SGD(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        "\n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    prev_grad_uw = [] # storing previous point\n",
        "    prev_grad_ub = [] # storing previous point\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers,batch_size,activation,loss_fun)\n",
        "      list_grad_w = add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "\n",
        "      if((image_no+1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        \n",
        "        if(len(prev_grad_uw) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            prev_grad_uw.append(beta * list_grad_w[i])\n",
        "            prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "          list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "        \n",
        "        theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "        b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "        \n",
        "        prev_grad_uw = list_u_grad_w.copy()\n",
        "        prev_grad_ub = list_u_grad_b.copy()\n",
        "        list_u_grad_w.clear()\n",
        "        list_u_grad_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    # Update theta for last few images\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      if(len(prev_grad_uw) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          prev_grad_uw.append(beta * list_grad_w[i])\n",
        "          prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "      for i in range(len(list_grad_w)):\n",
        "        list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "        list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "      \n",
        "      theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "      b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "      \n",
        "      prev_grad_uw = list_u_grad_w.copy()\n",
        "      prev_grad_ub = list_u_grad_b.copy()\n",
        "      list_u_grad_w.clear()\n",
        "      list_u_grad_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "  return(theta,b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7Gz1W23EO7U"
      },
      "outputs": [],
      "source": [
        "def RMS_SGD(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        " \n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    prev_grad_uw = [] # storing previous point\n",
        "    prev_grad_ub = [] # storing previous point\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train) \n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)\n",
        "      list_grad_w=add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b=add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "\n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        if(len(prev_grad_uw) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w.append((1 - beta) * list_grad_w[i] ** 2)\n",
        "            list_u_grad_b.append((1 - beta) * list_grad_b[i] ** 2)\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i] ** 2)\n",
        "            list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i] ** 2)\n",
        "        \n",
        "        for i in range(0,len(list_grad_w)):\n",
        "          theta[i] -= (eta * list_grad_w[i].transpose())/(list_u_grad_w[i].transpose() + 1e-6) ** 0.5\n",
        "          b[i] -= (eta * list_grad_b[i].transpose())/(list_u_grad_b[i].transpose()+ 1e-6) ** 0.5\n",
        "        \n",
        "        prev_grad_uw = list_u_grad_w.copy()\n",
        "        prev_grad_ub = list_u_grad_b.copy()\n",
        "        list_u_grad_w.clear()\n",
        "        list_u_grad_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      if(len(prev_grad_uw) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          prev_grad_uw.append(beta * list_grad_w[i])\n",
        "          prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "      for i in range(len(list_grad_w)):\n",
        "        list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i] ** 2)\n",
        "        list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i] ** 2)\n",
        "      \n",
        "      for i in range(0,len(list_grad_w)):\n",
        "        theta[i] -= (eta * list_grad_w[i].transpose())/(list_u_grad_w[i].transpose() + 1e-6) ** 0.5\n",
        "        b[i] -= (eta * list_grad_b[i].transpose())/(list_u_grad_b[i].transpose()+ 1e-6) ** 0.5\n",
        "      \n",
        "      prev_grad_uw = list_u_grad_w.copy()\n",
        "      prev_grad_ub = list_u_grad_b.copy()\n",
        "      list_u_grad_w.clear()\n",
        "      list_u_grad_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "  return(theta,b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d67UB_mGtGb"
      },
      "outputs": [],
      "source": [
        "def adam_SGD(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        "  \n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    list_m_w = [] # storing previous point\n",
        "    list_m_b = [] # storing previous point\n",
        "    list_v_w = [] # storing previous point\n",
        "    list_v_b = [] # storing previous point\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while( image_no < len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)\n",
        "      list_grad_w=add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b=add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      \n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        if(len(list_m_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "            list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "            list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "        \n",
        "        if(len(list_v_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "            list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "            list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "        \n",
        "        list_mhat_w = []\n",
        "        list_mhat_b = []\n",
        "        list_vhat_w = []\n",
        "        list_vhat_b = []\n",
        "        for i in range(len(list_m_w)):\n",
        "          list_mhat_w.append(list_m_w[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_mhat_b.append(list_m_b[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_w.append(list_v_w[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_b.append(list_v_b[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "\n",
        "        \n",
        "        # print(theta)\n",
        "        for i in range(0,len(list_grad_w)):\n",
        "          theta[i] -= (eta * list_mhat_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "          b[i] -= (eta * list_mhat_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "        \n",
        "        list_mhat_w.clear()\n",
        "        list_mhat_b.clear()\n",
        "        list_vhat_w.clear()\n",
        "        list_vhat_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      if(len(list_m_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "          list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "          list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "      \n",
        "      if(len(list_v_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "          list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "          list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "      \n",
        "      list_mhat_w = []\n",
        "      list_mhat_b = []\n",
        "      list_vhat_w = []\n",
        "      list_vhat_b = []\n",
        "      for i in range(len(list_m_w)):\n",
        "        list_mhat_w.append(list_m_w[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_mhat_b.append(list_m_b[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_vhat_w.append(list_v_w[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "        list_vhat_b.append(list_v_b[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "\n",
        "      \n",
        "      # print(theta)\n",
        "      for i in range(0,len(list_grad_w)):\n",
        "        theta[i] -= (eta * list_mhat_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "        b[i] -= (eta * list_mhat_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "      \n",
        "      list_mhat_w.clear()\n",
        "      list_mhat_b.clear()\n",
        "      list_vhat_w.clear()\n",
        "      list_vhat_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "  return(theta,b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZmDJQ4BfEpt"
      },
      "outputs": [],
      "source": [
        "def NAG(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        "  \n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    prev_grad_uw = [] # storing previous point\n",
        "    prev_grad_ub = [] # storing previous point\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      w_lookahead = []\n",
        "      b_lookahead = []\n",
        "      if(len(prev_grad_uw)==0):\n",
        "        w_lookahead = theta.copy()\n",
        "        b_lookahead = b.copy();\n",
        "      else :\n",
        "        for i in range(len(prev_grad_uw)):\n",
        "          w_lookahead.append(theta[i] - beta * prev_grad_uw[i].transpose())\n",
        "          b_lookahead.append(b[i] - beta * prev_grad_ub[i].transpose())\n",
        "      \n",
        "      fp_list = forward_prop(activation,w_lookahead,b_lookahead,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train) \n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "      bp_list = backward_prop(w_lookahead,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)\n",
        "      list_grad_w = add_list_grad(list_grad_w.copy() , bp_list[0])\n",
        "      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      w_lookahead.clear()\n",
        "      b_lookahead.clear()\n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        if(len(prev_grad_uw) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w = list_grad_w.copy()\n",
        "            list_u_grad_b = list_grad_b.copy()\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "            list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "        \n",
        "        # print(theta)\n",
        "        theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "        b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "        \n",
        "        prev_grad_uw = list_u_grad_w.copy()\n",
        "        prev_grad_ub = list_u_grad_b.copy()\n",
        "        list_u_grad_w.clear()\n",
        "        list_u_grad_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "        \n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      if(len(prev_grad_uw) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          prev_grad_uw.append(beta * list_grad_w[i])\n",
        "          prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "      for i in range(len(list_grad_w)):\n",
        "        list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "        list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "      \n",
        "      theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "      b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "      \n",
        "      prev_grad_uw = list_u_grad_w.copy()\n",
        "      prev_grad_ub = list_u_grad_b.copy()\n",
        "      list_u_grad_w.clear()\n",
        "      list_u_grad_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "  return(theta,b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-M3MQXIuGbo"
      },
      "outputs": [],
      "source": [
        "def nadam(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        "\n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    list_m_w = [] # storing previous point\n",
        "    list_m_b = [] # storing previous point\n",
        "    list_v_w = [] # storing previous point\n",
        "    list_v_b = [] # storing previous point\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)\n",
        "      list_grad_w=add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b=add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      \n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        if(len(list_m_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "            list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "            list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "        \n",
        "        if(len(list_v_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "            list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "            list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "        \n",
        "        list_mhat_w = []\n",
        "        list_mhat_b = []\n",
        "        list_vhat_w = []\n",
        "        list_vhat_b = []\n",
        "        for i in range(len(list_m_w)):\n",
        "          list_mhat_w.append(list_m_w[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_mhat_b.append(list_m_b[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_w.append(list_v_w[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_b.append(list_v_b[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "\n",
        "        \n",
        "        for i in range(0,len(list_grad_w)):\n",
        "          theta[i] -= (eta * list_mhat_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "          b[i] -= (eta * list_mhat_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "        \n",
        "        list_mhat_w.clear()\n",
        "        list_mhat_b.clear()\n",
        "        list_vhat_w.clear()\n",
        "        list_vhat_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      if(len(list_m_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "          list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "          list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "      \n",
        "      if(len(list_v_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "          list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "          list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "      \n",
        "      list_mhat_w = []\n",
        "      list_mhat_b = []\n",
        "      list_vhat_w = []\n",
        "      list_vhat_b = []\n",
        "      for i in range(len(list_m_w)):\n",
        "        list_mhat_w.append(list_m_w[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_mhat_b.append(list_m_b[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_vhat_w.append(list_v_w[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "        list_vhat_b.append(list_v_b[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "\n",
        "      nadam_term_w = []\n",
        "      nadam_term_b = []\n",
        "      for i in range(len(list_grad_w)):\n",
        "        nadam_term_w.append( beta1 * list_mhat_w[i] + ( (1 - beta1) * list_grad_w[i]) / (1 - beta1 ** int(image_no /batch_size) ))\n",
        "        nadam_term_b.append( beta1 * list_mhat_b[i] + ( (1 - beta1) * list_grad_b[i]) / (1 - beta1 ** int(image_no /batch_size) ))\n",
        "      # print(theta)\n",
        "      for i in range(0,len(list_grad_w)):\n",
        "        theta[i] -= (eta * nadam_term_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "        b[i] -= (eta * nadam_term_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "      \n",
        "      nadam_term_w.clear()\n",
        "      nadam_term_b.clear()\n",
        "      list_mhat_w.clear()\n",
        "      list_mhat_b.clear()\n",
        "      list_vhat_w.clear()\n",
        "      list_vhat_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "  return(theta,b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1eNrF00vC9A"
      },
      "outputs": [],
      "source": [
        "#hyperparameters\n",
        "\n",
        "# LEARNING_RATE = 0.001\n",
        "# ACTIVATION = \"sigmoid\"\n",
        "# INITIALIZER = \"xavier\"\n",
        "# OPTIMIZER = \"nag\"\n",
        "# BATCH_SIZE = 32\n",
        "# EPOCHS = 10\n",
        "# L2_lambda = 0.0005\n",
        "# LAYER_DIMS = [784,64,32,64,10]\n",
        "\n",
        "# LOSS = 'categorical_crossentropy'\n",
        "\n",
        "\n",
        "learning_rate = 0.1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "activation = \"tanh\"\n",
        "initializer = \"xavier\"\n",
        "optimizer = \"nag\"\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "loss_fun = \"cross_entropy\"\n",
        "neurons = 128\n",
        "num_layers = 2\n",
        "weight_decay = 0.0005\n",
        "# --- Accuracy = 97.57\n",
        "\n",
        "\n",
        "# learning_rate = 0.1\n",
        "# beta1 = 0.9\n",
        "# beta2 = 0.999\n",
        "# activation = \"relu\"\n",
        "# initializer = \"xavier\"\n",
        "# optimizer = \"sgd\"\n",
        "# batch_size = 32\n",
        "# epochs = 10\n",
        "# loss_fun = \"cross_entropy\"\n",
        "# neurons = 64\n",
        "# num_layers = 3\n",
        "# weight_decay = 0.0005\n",
        "# --- Accuracy = 96.78\n",
        "\n",
        "# learning_rate = 0.1\n",
        "# beta1 = 0.9\n",
        "# beta2 = 0.999\n",
        "# activation = \"tanh\"\n",
        "# initializer = \"xavier\"\n",
        "# optimizer = \"momentum\"\n",
        "# batch_size = 16\n",
        "# epochs = 10\n",
        "# loss_fun = \"cross_entropy\"\n",
        "# neurons = 32\n",
        "# num_layers = 2\n",
        "# weight_decay = 0.0005\n",
        "# --- Accuracy = 96.61"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWc5Sxa1o_J5"
      },
      "outputs": [],
      "source": [
        "def neural_network(x_train,y_train,x_test,y_test,eta,beta1,beta2,activation,initializer,optimizer,batch_size,epochs,loss_fun,weight_decay,num_layers,neurons):\n",
        "  neurons = [neurons]*(num_layers+2)\n",
        "  neurons[0] = 784\n",
        "  neurons[-1] = 10\n",
        "  if(initializer == \"xavier\"):\n",
        "    theta = initilize_theta_xavier(num_layers,neurons)\n",
        "    b = initilize_bias(num_layers,neurons)\n",
        "  elif (initializer == \"random_normal\"):\n",
        "    theta = initilize_theta_random_normal(num_layers,neurons)\n",
        "    b = initilize_bias(num_layers,neurons)\n",
        "  elif (initializer == \"random_uniform\"):\n",
        "    theta = initilize_theta_random_uniform(num_layers,neurons)\n",
        "    b = initilize_bias(num_layers,neurons)\n",
        "  else :\n",
        "    print(\"Invalid Intilizer\")\n",
        "  \n",
        "  # invoking training fun\n",
        "  if(optimizer == \"sgd\"):\n",
        "    theta,b = SGD(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  elif(optimizer == \"momentum\"):\n",
        "    theta,b = momentum_SGD(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  elif(optimizer == \"rms\"):\n",
        "    theta,b = RMS_SGD(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  elif(optimizer == \"adam\"):\n",
        "    theta,b = adam_SGD(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  elif(optimizer == \"nag\"):\n",
        "    theta,b = NAG(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  elif(optimizer == \"nadam\"):\n",
        "    theta,b = nadam(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  else :\n",
        "    print(\"Invalid Optimizer\")\n",
        "\n",
        "\n",
        "  return(theta,b)\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC8P9Y4RC73D",
        "outputId": "06191812-0270-4457-882c-1df5fa62a605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "training loss =  0.35777771677223563\n",
            "validation loss =  0.8269417043628999\n",
            "trianing accuracy =  93.34074074074074\n",
            "validation_accuracy =  93.13333333333334\n",
            "2\n",
            "training loss =  0.21385658783050218\n",
            "validation loss =  0.2447879062244716\n",
            "trianing accuracy =  95.16851851851852\n",
            "validation_accuracy =  94.86666666666666\n",
            "3\n",
            "training loss =  0.16178830747069003\n",
            "validation loss =  0.1816504519707208\n",
            "trianing accuracy =  96.24259259259259\n",
            "validation_accuracy =  95.88333333333334\n",
            "4\n",
            "training loss =  0.13099651052676628\n",
            "validation loss =  0.1485737400344\n",
            "trianing accuracy =  96.94814814814815\n",
            "validation_accuracy =  96.38333333333334\n",
            "5\n",
            "training loss =  0.11050319653309694\n",
            "validation loss =  0.12899900923627014\n",
            "trianing accuracy =  97.4074074074074\n",
            "validation_accuracy =  96.61666666666666\n",
            "6\n",
            "training loss =  0.09583658009386692\n",
            "validation loss =  0.11606702757653399\n",
            "trianing accuracy =  97.78148148148148\n",
            "validation_accuracy =  96.78333333333333\n",
            "7\n",
            "training loss =  0.08483527106102953\n",
            "validation loss =  0.10708158721122533\n",
            "trianing accuracy =  98.04074074074074\n",
            "validation_accuracy =  97.06666666666666\n",
            "8\n",
            "training loss =  0.07622654782930793\n",
            "validation loss =  0.10053524248320929\n",
            "trianing accuracy =  98.21851851851852\n",
            "validation_accuracy =  97.18333333333334\n",
            "9\n",
            "training loss =  0.06927019141984204\n",
            "validation loss =  0.09561968761675556\n",
            "trianing accuracy =  98.42592592592592\n",
            "validation_accuracy =  97.23333333333333\n",
            "10\n",
            "training loss =  0.06351280412447347\n",
            "validation loss =  0.09187764966388769\n",
            "trianing accuracy =  98.56666666666666\n",
            "validation_accuracy =  97.26666666666667\n"
          ]
        }
      ],
      "source": [
        "theta , b =neural_network(x_train,y_train,x_test,y_test,learning_rate,beta1,beta2,activation,initializer,optimizer,batch_size,epochs,loss_fun,weight_decay,num_layers,neurons)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ncr6h7igElO1",
        "outputId": "f643bd36-e0af-4598-be66-67fc0b4148ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "97.37\n"
          ]
        }
      ],
      "source": [
        " # testing accuracy\n",
        "accuracy = image_pridiction(x_test, y_test, theta, b, num_layers,activation)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-6qND3HuJGEd"
      },
      "outputs": [],
      "source": [
        "# def NN_fit():\n",
        "#     # Default values for hyper-parameters\n",
        "#     # config_defaults = {\n",
        "#     #     'epochs': 10,\n",
        "#     #     'batch_size': 64,\n",
        "#     #     'learning_rate': 1e-2,\n",
        "#     #     'activation_f': 'relu',\n",
        "#     #     'optimizer': 'adam',\n",
        "#     #     'init_mode': 'xavier',\n",
        "#     #     'L2_lamb': 0,\n",
        "#     #     'num_neurons': 32,\n",
        "#     #     'num_hidden': 2\n",
        "#     # }\n",
        "\n",
        "#     # Initialize a new wandb run\n",
        "#     wandb.init()\n",
        "    \n",
        "#     # Config is a variable that holds and saves hyperparameters and inputs\n",
        "#     config = wandb.config\n",
        "\n",
        "#     # Local variables, values obtained from wandb config\n",
        "#     num_neurons = config.num_neurons\n",
        "#     num_hidden = config.num_hidden\n",
        "#     init_mode = config.init_mode\n",
        "#     epochs = config.epochs\n",
        "#     batch_size = config.batch_size\n",
        "#     learning_rate = config.learning_rate\n",
        "#     activation_f = config.activation_f\n",
        "#     L2_lamb = config.L2_lamb\n",
        "#     optimizer = config.optimizer\n",
        "\n",
        "#     # Display the hyperparameters\n",
        "#     run_name = \"lr_{}_ac_{}_in_{}_op_{}_bs_{}_L2_{}_ep_{}_nn_{}_nh_{}\".format(learning_rate, activation_f, init_mode, optimizer, batch_size, L2_lamb, epochs, num_neurons, num_hidden)\n",
        "#     print(run_name)\n",
        "#     neural_network(x_train,y_train,x_test,y_test,learning_rate,0.9,0.999,activation_f,init_mode,optimizer,batch_size,epochs,\"mse\",0.0005,num_hidden,num_neurons)\n",
        "#     wandb.run.name = run_name\n",
        "#     wandb.run.save()\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2npDhTfaDH7V"
      },
      "outputs": [],
      "source": [
        "# sweep_config = {\n",
        "#   \"name\": \"CS6910 Assignment 1 - Squared Error Loss\",\n",
        "#   \"metric\": {\n",
        "#       \"name\":\"validation_accuracy\",\n",
        "#       \"goal\": \"maximize\"\n",
        "#   },\n",
        "#   \"method\": \"random\",\n",
        "#   \"parameters\": {\n",
        "#         \"learning_rate\": {\n",
        "#             \"values\": [0.1, 0.01]\n",
        "#         },\n",
        "#         \"activation_f\": {\n",
        "#             \"values\": [\"sigmoid\", \"relu\", \"tanh\"]\n",
        "#         },\n",
        "#         \"init_mode\": {\n",
        "#             \"values\": [\"xavier\", \"random_normal\"]\n",
        "#         },\n",
        "#         \"optimizer\": {\n",
        "#             \"values\": [\"sgd\", \"momentum\", \"nag\", \"adam\", \"nadam\", \"rms\"]\n",
        "#         },\n",
        "#         \"batch_size\": {\n",
        "#             \"values\": [16,32,64]\n",
        "#         },\n",
        "#         \"epochs\": {\n",
        "#             \"values\": [5]\n",
        "#         },\n",
        "#         \"L2_lamb\": {\n",
        "#             \"values\": [0, 0.0005, 0.5]\n",
        "#         },\n",
        "#         \"num_neurons\": {\n",
        "#             \"values\": [32,64,128]\n",
        "#         },\n",
        "#         \"num_hidden\": {\n",
        "#             \"values\": [2,3,4]\n",
        "#         }\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# sweep_id = wandb.sweep(sweep_config, entity=\"rajmahajan24\", project=\"DL_A1_optimizers_combined_13_01(mse)\")\n",
        "# wandb.agent(sweep_id, NN_fit, count=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuN06PEiCcJg"
      },
      "outputs": [],
      "source": [
        "# wandb.log({\"training_acc\": train_acc, \"validation_accuracy\": val_acc, \"training_loss\": cost, \"validation cost\": val_cost, 'epoch': count})\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}