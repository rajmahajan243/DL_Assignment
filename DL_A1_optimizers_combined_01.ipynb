{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poDfvogCn0d-"
      },
      "outputs": [],
      "source": [
        "# Name - Raj Mahajan\n",
        "# Roll Number - CS22M067\n",
        "# Assignment 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk8K1FuZUq-N",
        "outputId": "8d63ce6f-9010-4ea9-a0b2-e1ceb4304243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.17.0-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.4.3)\n",
            "Collecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=19b9448e374ea3f613f30ee181bbe22bb050fdd2138dcc8a2a418c396bf87e75\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.17.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login(key = \"c28eda24dacbd50a57332e445cfdfb2f4c16799c\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "j1QBwpKXU1_W",
        "outputId": "fc2a8153-b32e-4ee1-fc0c-970e6797cfe3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajmahajan24\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230317_060859-gzqg21n6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb/runs/gzqg21n6' target=\"_blank\">magic-sunset-1</a></strong> to <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb/runs/gzqg21n6' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb/runs/gzqg21n6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb/runs/gzqg21n6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7ff323e31280>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.init(project=\"DL_A1_optimizers_combined_14.ipynb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRTp4NxgpbsO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBFE0h1Hp-k0",
        "outputId": "3e30d81f-d059-4ca4-d422-33b5792aad04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(60000, 1)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the Fashion-MNIST dataset\n",
        "(X_train, Y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "X_train=X_train.reshape(X_train.shape[0],784)\n",
        "x_test=x_test.reshape(x_test.shape[0],784)\n",
        "Y_train=Y_train.reshape(60000,1)\n",
        "y_test=y_test.reshape(10000,1)\n",
        "# X_train.shape\n",
        "Y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgZvKQQ4qDJ-",
        "outputId": "ec9ecaa3-8062-4856-bb69-7b0cd86e2993"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 1)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Normalizing the data\n",
        "X_train=X_train/255.0\n",
        "x_test=x_test/255.0\n",
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPQgAvW_gO_w",
        "outputId": "10c0f45c-2879-4987-9d84-5d60696203ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6000, 784)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train,x_val,y_train,y_val = train_test_split(X_train , Y_train , test_size = 0.1 , random_state = 100)\n",
        "# x_train.shape\n",
        "x_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAuDaw6bo_Bu"
      },
      "outputs": [],
      "source": [
        "def actual_prob_dist(y) :\n",
        "  ans=np.zeros((10, 1))\n",
        "  ans[y]=1\n",
        "  return(ans)\n",
        "\n",
        "def sigmoid(a) :\n",
        "  # s=1/(1+np.exp(a))\n",
        "\n",
        "  if isinstance(a,np.ndarray):\n",
        "    return 1.0 / (1.0 + np.exp(-a))\n",
        "  else :\n",
        "    if (x>=0):\n",
        "      return 1.0 / (1.0 + np.exp(-a))\n",
        "    else :\n",
        "      return np.exp(a) / ( 1.0 + np.exp(a))\n",
        "  # return(s)\n",
        "\n",
        "\n",
        "\n",
        "def derivative_sigmoid(x):\n",
        "  ans = sigmoid(x) * (1 -sigmoid(x))\n",
        "  return(ans)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def derivative_relu(x):\n",
        "    return 1*(x>0)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def derivative_tanh(x):\n",
        "    return (1 - (np.tanh(x)**2))\n",
        "\n",
        "def softmax(v):\n",
        "  # ans = np.exp(v) / np.sum(np.exp(v), axis=0)\n",
        "  z = v - max(v)\n",
        "  numerator = np.exp(z)\n",
        "  denominator = np.sum(numerator)\n",
        "  ans = numerator/denominator\n",
        "  return(ans)\n",
        "\n",
        "\n",
        "def derivative_softmax(x):\n",
        "  ans= softmax(x) * (1-softmax(x))\n",
        "  return(ans)\n",
        "\n",
        "\n",
        "def cross_entropy(y, yhat, batch_size):\n",
        "   loss = (-1.0 * np.sum(np.multiply(y, np.log(yhat))))/batch_size\n",
        "   return(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQc_Udj0o_DW"
      },
      "outputs": [],
      "source": [
        "def add_list_grad(list_grad_x , list_grad_y):\n",
        "  if(len(list_grad_x) == 0 ):\n",
        "    return(list_grad_y)\n",
        "  for i in range(0,len(list_grad_x)):\n",
        "    list_grad_x[i] += list_grad_y[i]\n",
        "  return(list_grad_x)\n",
        "\n",
        "def add_list_grad_eta(list_grad_x , list_grad_y, eta):\n",
        "  if(len(list_grad_x) == 0 ):\n",
        "    return(list_grad_y)\n",
        "  for i in range(0,len(list_grad_x)):\n",
        "    list_grad_x[i] -= eta * list_grad_y[i].transpose()\n",
        "  return(list_grad_x)\n",
        "\n",
        "def initilize_theta_random_normal(num_layers,neurons):\n",
        "  theta=[]\n",
        "  i=0\n",
        "  temp=np.random.randn(neurons[0], neurons[1])\n",
        "  theta.append(temp)\n",
        "  while i< len(neurons)-3 :\n",
        "    temp=np.random.randn(neurons[i+1], neurons[i+2])\n",
        "    theta.append(temp)\n",
        "    i+=1\n",
        "  temp=np.random.randn(neurons[-2], neurons[-1])\n",
        "  theta.append(temp)\n",
        "  return(theta)\n",
        "\n",
        "def initilize_theta_random_uniform(num_layers,neurons):\n",
        "  theta=[]\n",
        "  i=0\n",
        "  temp=np.random.rand(neurons[0], neurons[1])\n",
        "  theta.append(temp)\n",
        "  while i< len(neurons)-3 :\n",
        "    temp=np.random.rand(neurons[i+1], neurons[i+2])\n",
        "    theta.append(temp)\n",
        "    i+=1\n",
        "  temp=np.random.rand(neurons[-2], neurons[-1])\n",
        "  theta.append(temp)\n",
        "  return(theta)\n",
        "\n",
        "def initilize_theta_xavier(num_layers,neurons):\n",
        "  theta=[]\n",
        "  i=0\n",
        "  temp=np.random.randn(neurons[0], neurons[1]) * np.sqrt(2/(neurons[0]  + neurons[1])) # xavier\n",
        "  theta.append(temp)\n",
        "  while i< len(neurons)-3 :\n",
        "    temp=np.random.randn(neurons[i+1], neurons[i+2]) * np.sqrt(2/(neurons[i+1] + neurons[i+2])) # xavier\n",
        "    theta.append(temp)\n",
        "    i+=1\n",
        "  temp=np.random.randn(neurons[-2], neurons[-1]) * np.sqrt(2/(neurons[-2] + neurons[-1])) # xavier\n",
        "  theta.append(temp)\n",
        "  return(theta)\n",
        "\n",
        "def initilize_bias(num_layers,neurons):\n",
        "  b=[]\n",
        "  i=0\n",
        "  while i < num_layers :\n",
        "    temp=np.zeros((1,neurons[i+1]))\n",
        "    b.append(temp)\n",
        "    i+=1\n",
        "  temp=np.zeros((1, neurons[-1]))\n",
        "  b.append(temp)\n",
        "  return(b)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJCVfydl8XrV"
      },
      "outputs": [],
      "source": [
        "def activation_fun(activation,a):\n",
        "  if(activation == \"sigmoid\"):\n",
        "    return(sigmoid(a))\n",
        "  elif(activation == \"relu\"):\n",
        "    return(relu(a))\n",
        "  elif(activation == \"tanh\"):\n",
        "    return(tanh(a))\n",
        "\n",
        "def derivative_activation_fun(activation,a):\n",
        "  if(activation == \"sigmoid\"):\n",
        "    return(derivative_sigmoid(a))\n",
        "  elif(activation == \"relu\"):\n",
        "    return(derivative_relu(a))\n",
        "  elif(activation == \"tanh\"):\n",
        "    return(derivative_tanh(a))\n",
        "  \n",
        "def loss_function(loss_fun,yhat,y):\n",
        "  if(loss_fun == \"cross_entropy\"):\n",
        "    loss = (-math.log(yhat[y]))\n",
        "  elif(loss_fun == \"mse\"):\n",
        "    Y = actual_prob_dist(y)\n",
        "    loss = (1/2) * np.sum((Y-yhat)**2)\n",
        "  return(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzS5d-JFo_E4"
      },
      "outputs": [],
      "source": [
        "# Constructed only for 1 image \n",
        "# h will be image input if 1d vector of length 28x28\n",
        "def forward_prop(activation,theta,b,num_layers,h):\n",
        "  h_list=[]\n",
        "  h_list.append(h)\n",
        "  a_list=[]\n",
        "  for k in range(0,num_layers):\n",
        "    a = b[k].transpose() + np.matmul(theta[k].transpose() , h)\n",
        "    a_list.append(a)\n",
        "    h= activation_fun(activation,a)\n",
        "    h_list.append(h)\n",
        "  # For last output layer\n",
        "  a = b[num_layers].transpose() + np.matmul(theta[num_layers].transpose() , h)\n",
        "  a_list.append(a)\n",
        "  yhat = softmax(a)\n",
        "  return(yhat,h_list,a_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7FjUsDzo_Gc"
      },
      "outputs": [],
      "source": [
        "# only for 1 image\n",
        "def backward_prop(theta,h_list,a_list,y,yhat,num_layers,batch_size,activation,loss_fun):\n",
        "  grad_a=-(actual_prob_dist(y) - yhat)\n",
        "  if(loss_fun == \"mse\"):\n",
        "      grad_a *= derivative_softmax(a_list[len(a_list)-1]);\n",
        "  list_grad_w=[]\n",
        "  list_grad_b=[]\n",
        "  for k in range(num_layers , -1,-1) :\n",
        "    grad_w = np.matmul(grad_a , h_list[k].transpose()) \n",
        "    list_grad_w.append(grad_w / batch_size)\n",
        "    grad_b = grad_a.copy()\n",
        "    list_grad_b.append(grad_b / batch_size)\n",
        "    if k >0:\n",
        "      grad_h = np.matmul(theta[k] , grad_a)\n",
        "      grad_a = grad_h * derivative_activation_fun(activation,a_list[k-1])\n",
        "  return(list_grad_w , list_grad_b)\n",
        "  \n",
        "\n",
        "# grad_b.shape = 32 x 1\n",
        "# grad_w.shape = 32 x 32\n",
        "# grad_a.shape = 10/32 x 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugPI1ENFuZIi"
      },
      "outputs": [],
      "source": [
        "def image_pridiction(x_test, y_test, theta, b, num_layers,activation):\n",
        "  img_no = 0\n",
        "  wrong_count = 0\n",
        "  while(img_no<len(x_test)):\n",
        "    img = x_test[img_no].reshape(784,1)\n",
        "    clas = y_test[img_no]\n",
        "    temp_ans = forward_prop(activation,theta,b,num_layers,img)\n",
        "    yhat = temp_ans[0]\n",
        "    temp = -1\n",
        "    predicted_class = -1\n",
        "    sum=0\n",
        "    for i in range(0 , 10):\n",
        "      sum+=yhat[i]\n",
        "      if(temp < yhat[i]):\n",
        "        predicted_class = i\n",
        "        temp = yhat[i]\n",
        "    if(clas != predicted_class):\n",
        "      wrong_count+=1\n",
        "    img_no+=1\n",
        "  accuracy = (len(x_test) - wrong_count) * 100 / len(x_test)\n",
        "  return(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3Jyo0kzr1jR"
      },
      "outputs": [],
      "source": [
        "def SGD(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t<epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "         \n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      y_hat,h_list,a_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # training loss\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers,batch_size,activation,loss_fun)\n",
        "      list_grad_w = add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        theta = add_list_grad_eta(theta.copy() , list_grad_w, eta)\n",
        "        b = add_list_grad_eta(b.copy(), list_grad_b , eta)\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      # print(theta)\n",
        "      image_no += 1\n",
        "\n",
        "    # Update theta for last few images\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      theta = add_list_grad_eta(theta.copy() , list_grad_w, eta)\n",
        "      b = add_list_grad_eta(b.copy(), list_grad_b , eta)\n",
        "\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "\n",
        "  return(theta,b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyITICU2r1UN"
      },
      "outputs": [],
      "source": [
        "def momentum_SGD(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        "\n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    prev_grad_uw = [] # storing previous point\n",
        "    prev_grad_ub = [] # storing previous point\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers,batch_size,activation,loss_fun)\n",
        "      list_grad_w = add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "\n",
        "      if((image_no+1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        \n",
        "        if(len(prev_grad_uw) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            prev_grad_uw.append(beta * list_grad_w[i])\n",
        "            prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "          list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "        \n",
        "        theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "        b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "        \n",
        "        prev_grad_uw = list_u_grad_w.copy()\n",
        "        prev_grad_ub = list_u_grad_b.copy()\n",
        "        list_u_grad_w.clear()\n",
        "        list_u_grad_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    # Update theta for last few images\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      if(len(prev_grad_uw) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          prev_grad_uw.append(beta * list_grad_w[i])\n",
        "          prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "      for i in range(len(list_grad_w)):\n",
        "        list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "        list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "      \n",
        "      theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "      b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "      \n",
        "      prev_grad_uw = list_u_grad_w.copy()\n",
        "      prev_grad_ub = list_u_grad_b.copy()\n",
        "      list_u_grad_w.clear()\n",
        "      list_u_grad_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "  return(theta,b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7Gz1W23EO7U"
      },
      "outputs": [],
      "source": [
        "def RMS_SGD(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        " \n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    prev_grad_uw = [] # storing previous point\n",
        "    prev_grad_ub = [] # storing previous point\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train) \n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)\n",
        "      list_grad_w=add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b=add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "\n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        if(len(prev_grad_uw) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w.append((1 - beta) * list_grad_w[i] ** 2)\n",
        "            list_u_grad_b.append((1 - beta) * list_grad_b[i] ** 2)\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i] ** 2)\n",
        "            list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i] ** 2)\n",
        "        \n",
        "        for i in range(0,len(list_grad_w)):\n",
        "          theta[i] -= (eta * list_grad_w[i].transpose())/(list_u_grad_w[i].transpose() + 1e-6) ** 0.5\n",
        "          b[i] -= (eta * list_grad_b[i].transpose())/(list_u_grad_b[i].transpose()+ 1e-6) ** 0.5\n",
        "        \n",
        "        prev_grad_uw = list_u_grad_w.copy()\n",
        "        prev_grad_ub = list_u_grad_b.copy()\n",
        "        list_u_grad_w.clear()\n",
        "        list_u_grad_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      if(len(prev_grad_uw) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          prev_grad_uw.append(beta * list_grad_w[i])\n",
        "          prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "      for i in range(len(list_grad_w)):\n",
        "        list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i] ** 2)\n",
        "        list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i] ** 2)\n",
        "      \n",
        "      for i in range(0,len(list_grad_w)):\n",
        "        theta[i] -= (eta * list_grad_w[i].transpose())/(list_u_grad_w[i].transpose() + 1e-6) ** 0.5\n",
        "        b[i] -= (eta * list_grad_b[i].transpose())/(list_u_grad_b[i].transpose()+ 1e-6) ** 0.5\n",
        "      \n",
        "      prev_grad_uw = list_u_grad_w.copy()\n",
        "      prev_grad_ub = list_u_grad_b.copy()\n",
        "      list_u_grad_w.clear()\n",
        "      list_u_grad_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "  return(theta,b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d67UB_mGtGb"
      },
      "outputs": [],
      "source": [
        "def adam_SGD(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        "  \n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    list_m_w = [] # storing previous point\n",
        "    list_m_b = [] # storing previous point\n",
        "    list_v_w = [] # storing previous point\n",
        "    list_v_b = [] # storing previous point\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while( image_no < len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)\n",
        "      list_grad_w=add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b=add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      \n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        if(len(list_m_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "            list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "            list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "        \n",
        "        if(len(list_v_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "            list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "            list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "        \n",
        "        list_mhat_w = []\n",
        "        list_mhat_b = []\n",
        "        list_vhat_w = []\n",
        "        list_vhat_b = []\n",
        "        for i in range(len(list_m_w)):\n",
        "          list_mhat_w.append(list_m_w[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_mhat_b.append(list_m_b[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_w.append(list_v_w[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_b.append(list_v_b[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "\n",
        "        \n",
        "        # print(theta)\n",
        "        for i in range(0,len(list_grad_w)):\n",
        "          theta[i] -= (eta * list_mhat_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "          b[i] -= (eta * list_mhat_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "        \n",
        "        list_mhat_w.clear()\n",
        "        list_mhat_b.clear()\n",
        "        list_vhat_w.clear()\n",
        "        list_vhat_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      if(len(list_m_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "          list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "          list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "      \n",
        "      if(len(list_v_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "          list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "          list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "      \n",
        "      list_mhat_w = []\n",
        "      list_mhat_b = []\n",
        "      list_vhat_w = []\n",
        "      list_vhat_b = []\n",
        "      for i in range(len(list_m_w)):\n",
        "        list_mhat_w.append(list_m_w[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_mhat_b.append(list_m_b[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_vhat_w.append(list_v_w[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "        list_vhat_b.append(list_v_b[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "\n",
        "      \n",
        "      # print(theta)\n",
        "      for i in range(0,len(list_grad_w)):\n",
        "        theta[i] -= (eta * list_mhat_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "        b[i] -= (eta * list_mhat_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "      \n",
        "      list_mhat_w.clear()\n",
        "      list_mhat_b.clear()\n",
        "      list_vhat_w.clear()\n",
        "      list_vhat_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "  return(theta,b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZmDJQ4BfEpt"
      },
      "outputs": [],
      "source": [
        "def NAG(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        "  \n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    prev_grad_uw = [] # storing previous point\n",
        "    prev_grad_ub = [] # storing previous point\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      w_lookahead = []\n",
        "      b_lookahead = []\n",
        "      if(len(prev_grad_uw)==0):\n",
        "        w_lookahead = theta.copy()\n",
        "        b_lookahead = b.copy();\n",
        "      else :\n",
        "        for i in range(len(prev_grad_uw)):\n",
        "          w_lookahead.append(theta[i] - beta * prev_grad_uw[i].transpose())\n",
        "          b_lookahead.append(b[i] - beta * prev_grad_ub[i].transpose())\n",
        "      \n",
        "      fp_list = forward_prop(activation,w_lookahead,b_lookahead,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train) \n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "      bp_list = backward_prop(w_lookahead,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)\n",
        "      list_grad_w = add_list_grad(list_grad_w.copy() , bp_list[0])\n",
        "      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      w_lookahead.clear()\n",
        "      b_lookahead.clear()\n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        if(len(prev_grad_uw) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w = list_grad_w.copy()\n",
        "            list_u_grad_b = list_grad_b.copy()\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "            list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "        \n",
        "        # print(theta)\n",
        "        theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "        b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "        \n",
        "        prev_grad_uw = list_u_grad_w.copy()\n",
        "        prev_grad_ub = list_u_grad_b.copy()\n",
        "        list_u_grad_w.clear()\n",
        "        list_u_grad_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "        \n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      if(len(prev_grad_uw) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          prev_grad_uw.append(beta * list_grad_w[i])\n",
        "          prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "      for i in range(len(list_grad_w)):\n",
        "        list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "        list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "      \n",
        "      theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "      b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "      \n",
        "      prev_grad_uw = list_u_grad_w.copy()\n",
        "      prev_grad_ub = list_u_grad_b.copy()\n",
        "      list_u_grad_w.clear()\n",
        "      list_u_grad_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "  return(theta,b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-M3MQXIuGbo"
      },
      "outputs": [],
      "source": [
        "def nadam(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        "\n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    list_m_w = [] # storing previous point\n",
        "    list_m_b = [] # storing previous point\n",
        "    list_v_w = [] # storing previous point\n",
        "    list_v_b = [] # storing previous point\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)\n",
        "      list_grad_w=add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b=add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      \n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        if(len(list_m_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "            list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "            list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "        \n",
        "        if(len(list_v_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "            list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "            list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "        \n",
        "        list_mhat_w = []\n",
        "        list_mhat_b = []\n",
        "        list_vhat_w = []\n",
        "        list_vhat_b = []\n",
        "        for i in range(len(list_m_w)):\n",
        "          list_mhat_w.append(list_m_w[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_mhat_b.append(list_m_b[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_w.append(list_v_w[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_b.append(list_v_b[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "\n",
        "        \n",
        "        for i in range(0,len(list_grad_w)):\n",
        "          theta[i] -= (eta * list_mhat_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "          b[i] -= (eta * list_mhat_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "        \n",
        "        list_mhat_w.clear()\n",
        "        list_mhat_b.clear()\n",
        "        list_vhat_w.clear()\n",
        "        list_vhat_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      if(len(list_m_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "          list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "          list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "      \n",
        "      if(len(list_v_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "          list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "          list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "      \n",
        "      list_mhat_w = []\n",
        "      list_mhat_b = []\n",
        "      list_vhat_w = []\n",
        "      list_vhat_b = []\n",
        "      for i in range(len(list_m_w)):\n",
        "        list_mhat_w.append(list_m_w[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_mhat_b.append(list_m_b[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_vhat_w.append(list_v_w[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "        list_vhat_b.append(list_v_b[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "\n",
        "      nadam_term_w = []\n",
        "      nadam_term_b = []\n",
        "      for i in range(len(list_grad_w)):\n",
        "        nadam_term_w.append( beta1 * list_mhat_w[i] + ( (1 - beta1) * list_grad_w[i]) / (1 - beta1 ** int(image_no /batch_size) ))\n",
        "        nadam_term_b.append( beta1 * list_mhat_b[i] + ( (1 - beta1) * list_grad_b[i]) / (1 - beta1 ** int(image_no /batch_size) ))\n",
        "      # print(theta)\n",
        "      for i in range(0,len(list_grad_w)):\n",
        "        theta[i] -= (eta * nadam_term_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "        b[i] -= (eta * nadam_term_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "      \n",
        "      nadam_term_w.clear()\n",
        "      nadam_term_b.clear()\n",
        "      list_mhat_w.clear()\n",
        "      list_mhat_b.clear()\n",
        "      list_vhat_w.clear()\n",
        "      list_vhat_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "  return(theta,b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1eNrF00vC9A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWc5Sxa1o_J5"
      },
      "outputs": [],
      "source": [
        "def neural_network(x_train,y_train,x_test,y_test,eta,beta1,beta2,activation,initializer,optimizer,batch_size,epochs,loss_fun,weight_decay,num_layers,neurons):\n",
        "  neurons = [neurons]*(num_layers+2)\n",
        "  neurons[0] = 784\n",
        "  neurons[-1] = 10\n",
        "  if(initializer == \"xavier\"):\n",
        "    theta = initilize_theta_xavier(num_layers,neurons)\n",
        "    b = initilize_bias(num_layers,neurons)\n",
        "  elif (initializer == \"random_normal\"):\n",
        "    theta = initilize_theta_random_normal(num_layers,neurons)\n",
        "    b = initilize_bias(num_layers,neurons)\n",
        "  elif (initializer == \"random_uniform\"):\n",
        "    theta = initilize_theta_random_uniform(num_layers,neurons)\n",
        "    b = initilize_bias(num_layers,neurons)\n",
        "  else :\n",
        "    print(\"Invalid Intilizer\")\n",
        "  \n",
        "  # invoking training fun\n",
        "  if(optimizer == \"sgd\"):\n",
        "    theta,b = SGD(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  elif(optimizer == \"momentum\"):\n",
        "    theta,b = momentum_SGD(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  elif(optimizer == \"rms\"):\n",
        "    theta,b = RMS_SGD(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  elif(optimizer == \"adam\"):\n",
        "    theta,b = adam_SGD(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  elif(optimizer == \"nag\"):\n",
        "    theta,b = NAG(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  elif(optimizer == \"nadam\"):\n",
        "    theta,b = nadam(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  else :\n",
        "    print(\"Invalid Optimizer\")\n",
        "\n",
        "\n",
        "  return(theta,b)\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC8P9Y4RC73D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncr6h7igElO1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6qND3HuJGEd"
      },
      "outputs": [],
      "source": [
        "def run_NN():\n",
        "    \n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init()\n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "\n",
        "    # Local variables, values obtained from wandb config\n",
        "    num_neurons = config.num_neurons\n",
        "    num_hidden = config.num_hidden\n",
        "    init_mode = config.init_mode\n",
        "    epochs = config.epochs\n",
        "    batch_size = config.batch_size\n",
        "    learning_rate = config.learning_rate\n",
        "    activation_f = config.activation_f\n",
        "    L2_lamb = config.L2_lamb\n",
        "    optimizer = config.optimizer\n",
        "\n",
        "    # Display the hyperparameters\n",
        "    run_name = \"lr_{}_ac_{}_in_{}_op_{}_bs_{}_L2_{}_ep_{}_nn_{}_nh_{}\".format(learning_rate, activation_f, init_mode, optimizer, batch_size, L2_lamb, epochs, num_neurons, num_hidden)\n",
        "    print(run_name)\n",
        "    neural_network(x_train,y_train,x_test,y_test,learning_rate,0.9,0.999,activation_f,init_mode,optimizer,batch_size,epochs,\"mse\",0.0005,num_hidden,num_neurons)\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dbf55f63f7f94916af4401a6ba590c5c",
            "a64c5e69a09d4ee7b5122c08c582ea94",
            "fc5e3d7072b84d7ba17a37e7a008ddaf",
            "fc4492939d4348a094471e7908fa31cf",
            "b7aef1a2015b44aea06eb83c3b3759c6",
            "2d71a7d4b838497bb1486ac0b57644c8",
            "35708804a04a458f9e1474425dffef7e"
          ]
        },
        "id": "2npDhTfaDH7V",
        "outputId": "7f6302de-f269-4c20-ffc9-89387c712c64"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: zs5wny9u\n",
            "Sweep URL: https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in thread NetStatThr:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.9/threading.py\", line 917, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py\", line 258, in check_network_status\n",
            "    self._loop_check_status(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py\", line 214, in _loop_check_status\n",
            "    local_handle = request()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/interface.py\", line 795, in deliver_network_status\n",
            "    return self._deliver_network_status(status)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/interface_shared.py\", line 601, in _deliver_network_status\n",
            "    return self._deliver_record(record)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/interface_shared.py\", line 560, in _deliver_record\n",
            "    handle = mailbox._deliver_record(record, interface=self)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
            "    interface._publish(record)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3w4icr3o with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tL2_lamb: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_f: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_mode: random_normal\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230317_060919-3w4icr3o</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/3w4icr3o' target=\"_blank\">stellar-sweep-1</a></strong> to <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/3w4icr3o' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/3w4icr3o</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr_0.01_ac_relu_in_random_normal_op_adam_bs_32_L2_0_ep_5_nn_64_nh_2\n",
            "1\n",
            "training loss =  0.2818688300925255\n",
            "validation loss =  0.4183552244436207\n",
            "trianing accuracy =  9.964814814814815\n",
            "validation_accuracy =  10.316666666666666\n",
            "2\n",
            "training loss =  0.5295248268806065\n",
            "validation loss =  0.7186153042152723\n",
            "trianing accuracy =  10.037037037037036\n",
            "validation_accuracy =  9.666666666666666\n",
            "3\n",
            "training loss =  0.6750144606105285\n",
            "validation loss =  0.7712773511636248\n",
            "trianing accuracy =  10.005555555555556\n",
            "validation_accuracy =  9.95\n",
            "4\n",
            "training loss =  0.7249056064466013\n",
            "validation loss =  0.7111098726904982\n",
            "trianing accuracy =  10.037037037037036\n",
            "validation_accuracy =  9.666666666666666\n",
            "5\n",
            "training loss =  0.5013144731432437\n",
            "validation loss =  0.7742752707967943\n",
            "trianing accuracy =  9.898148148148149\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation_accuracy =  10.916666666666666\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_accuracy</td><td>▄█▆█▁</td></tr><tr><td>training_loss</td><td>▁▅▇█▄</td></tr><tr><td>validation cost</td><td>▁▇█▇█</td></tr><tr><td>validation_accuracy</td><td>▅▁▃▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_accuracy</td><td>9.89815</td></tr><tr><td>training_loss</td><td>0.50131</td></tr><tr><td>validation cost</td><td>0.77428</td></tr><tr><td>validation_accuracy</td><td>10.91667</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stellar-sweep-1</strong> at: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/3w4icr3o' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/3w4icr3o</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230317_060919-3w4icr3o/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ygwha0c8 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tL2_lamb: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_f: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_mode: xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230317_061526-ygwha0c8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/ygwha0c8' target=\"_blank\">summer-sweep-2</a></strong> to <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/ygwha0c8' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/ygwha0c8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr_0.1_ac_sigmoid_in_xavier_op_nadam_bs_16_L2_0.0005_ep_5_nn_128_nh_3\n",
            "1\n",
            "training loss =  0.8904127635970599\n",
            "validation loss =  0.8520898445528095\n",
            "trianing accuracy =  10.035185185185185\n",
            "validation_accuracy =  9.683333333333334\n",
            "2\n",
            "training loss =  0.8331395847385884\n",
            "validation loss =  0.6538492361078106\n",
            "trianing accuracy =  10.035185185185185\n",
            "validation_accuracy =  9.683333333333334\n",
            "3\n",
            "training loss =  0.8505401460428471\n",
            "validation loss =  0.6428778154389143\n",
            "trianing accuracy =  10.055555555555555\n",
            "validation_accuracy =  9.5\n",
            "4\n",
            "training loss =  0.8214967583861597\n",
            "validation loss =  0.6523164227409156\n",
            "trianing accuracy =  9.898148148148149\n",
            "validation_accuracy =  10.916666666666666\n",
            "5\n",
            "training loss =  0.8973560272850616\n",
            "validation loss =  0.8970464249352149\n",
            "trianing accuracy =  9.898148148148149\n",
            "validation_accuracy =  10.916666666666666\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbf55f63f7f94916af4401a6ba590c5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_accuracy</td><td>▇▇█▁▁</td></tr><tr><td>training_loss</td><td>▇▂▄▁█</td></tr><tr><td>validation cost</td><td>▇▁▁▁█</td></tr><tr><td>validation_accuracy</td><td>▂▂▁██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_accuracy</td><td>9.89815</td></tr><tr><td>training_loss</td><td>0.89736</td></tr><tr><td>validation cost</td><td>0.89705</td></tr><tr><td>validation_accuracy</td><td>10.91667</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">summer-sweep-2</strong> at: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/ygwha0c8' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/ygwha0c8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230317_061526-ygwha0c8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fdanjuvi with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tL2_lamb: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_f: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_mode: xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230317_062747-fdanjuvi</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/fdanjuvi' target=\"_blank\">ruby-sweep-3</a></strong> to <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/fdanjuvi' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/fdanjuvi</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr_0.01_ac_sigmoid_in_xavier_op_sgd_bs_64_L2_0.5_ep_5_nn_32_nh_2\n",
            "1\n",
            "training loss =  0.4584314747516075\n",
            "validation loss =  0.4667982514235554\n",
            "trianing accuracy =  9.011111111111111\n",
            "validation_accuracy =  9.55\n",
            "2\n",
            "training loss =  0.45234869009094997\n",
            "validation loss =  0.4541585734650125\n",
            "trianing accuracy =  16.81851851851852\n",
            "validation_accuracy =  17.516666666666666\n",
            "3\n",
            "training loss =  0.45022050920343076\n",
            "validation loss =  0.45124434571192445\n",
            "trianing accuracy =  26.21851851851852\n",
            "validation_accuracy =  26.733333333333334\n",
            "4\n",
            "training loss =  0.44869406523889277\n",
            "validation loss =  0.449473780553679\n",
            "trianing accuracy =  27.761111111111113\n",
            "validation_accuracy =  27.566666666666666\n",
            "5\n",
            "training loss =  0.4474556050374836\n",
            "validation loss =  0.4480613022033195\n",
            "trianing accuracy =  27.461111111111112\n",
            "validation_accuracy =  27.033333333333335\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a64c5e69a09d4ee7b5122c08c582ea94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.009 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.070184…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_accuracy</td><td>▁▄▇██</td></tr><tr><td>training_loss</td><td>█▄▃▂▁</td></tr><tr><td>validation cost</td><td>█▃▂▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▄███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_accuracy</td><td>27.46111</td></tr><tr><td>training_loss</td><td>0.44746</td></tr><tr><td>validation cost</td><td>0.44806</td></tr><tr><td>validation_accuracy</td><td>27.03333</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ruby-sweep-3</strong> at: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/fdanjuvi' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/fdanjuvi</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230317_062747-fdanjuvi/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 57e2fe7l with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tL2_lamb: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_f: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_mode: random_normal\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230317_063219-57e2fe7l</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/57e2fe7l' target=\"_blank\">earthy-sweep-4</a></strong> to <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/57e2fe7l' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/57e2fe7l</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr_0.1_ac_tanh_in_random_normal_op_nag_bs_32_L2_0_ep_5_nn_64_nh_4\n",
            "1\n",
            "training loss =  0.45839023366428333\n",
            "validation loss =  0.7568979878896659\n",
            "trianing accuracy =  43.85740740740741\n",
            "validation_accuracy =  43.43333333333333\n",
            "2\n",
            "training loss =  0.31521022893295103\n",
            "validation loss =  0.3534126103907524\n",
            "trianing accuracy =  57.385185185185186\n",
            "validation_accuracy =  57.166666666666664\n",
            "3\n",
            "training loss =  0.2650880523560954\n",
            "validation loss =  0.27963219685086926\n",
            "trianing accuracy =  62.379629629629626\n",
            "validation_accuracy =  62.35\n",
            "4\n",
            "training loss =  0.23968830403016989\n",
            "validation loss =  0.2494492750544016\n",
            "trianing accuracy =  65.25185185185185\n",
            "validation_accuracy =  65.11666666666666\n",
            "5\n",
            "training loss =  0.22147081810935296\n",
            "validation loss =  0.23157771864026258\n",
            "trianing accuracy =  68.2537037037037\n",
            "validation_accuracy =  67.78333333333333\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc5e3d7072b84d7ba17a37e7a008ddaf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.009 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.070177…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇█</td></tr><tr><td>training_loss</td><td>█▄▂▂▁</td></tr><tr><td>validation cost</td><td>█▃▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_accuracy</td><td>68.2537</td></tr><tr><td>training_loss</td><td>0.22147</td></tr><tr><td>validation cost</td><td>0.23158</td></tr><tr><td>validation_accuracy</td><td>67.78333</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">earthy-sweep-4</strong> at: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/57e2fe7l' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/57e2fe7l</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230317_063219-57e2fe7l/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 13f69jyv with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tL2_lamb: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_f: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_mode: xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230317_064045-13f69jyv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/13f69jyv' target=\"_blank\">wild-sweep-5</a></strong> to <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/13f69jyv' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/13f69jyv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr_0.1_ac_sigmoid_in_xavier_op_nadam_bs_64_L2_0_ep_5_nn_32_nh_3\n",
            "1\n",
            "training loss =  0.46656646299428256\n",
            "validation loss =  0.46554936277752645\n",
            "trianing accuracy =  10.055555555555555\n",
            "validation_accuracy =  9.5\n",
            "2\n",
            "training loss =  0.46715388889399123\n",
            "validation loss =  0.4760164703487918\n",
            "trianing accuracy =  9.898148148148149\n",
            "validation_accuracy =  10.916666666666666\n",
            "3\n",
            "training loss =  0.4670677033237221\n",
            "validation loss =  0.4754276433321897\n",
            "trianing accuracy =  9.975925925925926\n",
            "validation_accuracy =  10.216666666666667\n",
            "4\n",
            "training loss =  0.468042540381913\n",
            "validation loss =  0.48714378768955824\n",
            "trianing accuracy =  9.975925925925926\n",
            "validation_accuracy =  10.216666666666667\n",
            "5\n",
            "training loss =  0.4686731024472198\n",
            "validation loss =  0.47952078419007993\n",
            "trianing accuracy =  9.898148148148149\n",
            "validation_accuracy =  10.916666666666666\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc4492939d4348a094471e7908fa31cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_accuracy</td><td>█▁▄▄▁</td></tr><tr><td>training_loss</td><td>▁▃▃▆█</td></tr><tr><td>validation cost</td><td>▁▄▄█▆</td></tr><tr><td>validation_accuracy</td><td>▁█▅▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_accuracy</td><td>9.89815</td></tr><tr><td>training_loss</td><td>0.46867</td></tr><tr><td>validation cost</td><td>0.47952</td></tr><tr><td>validation_accuracy</td><td>10.91667</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wild-sweep-5</strong> at: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/13f69jyv' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/13f69jyv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230317_064045-13f69jyv/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p9tubfcw with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tL2_lamb: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_f: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_mode: random_normal\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230317_064546-p9tubfcw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/p9tubfcw' target=\"_blank\">amber-sweep-6</a></strong> to <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/p9tubfcw' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/p9tubfcw</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr_0.01_ac_sigmoid_in_random_normal_op_momentum_bs_32_L2_0.5_ep_5_nn_32_nh_2\n",
            "1\n",
            "training loss =  0.5157310615453514\n",
            "validation loss =  0.6373261734952667\n",
            "trianing accuracy =  23.42962962962963\n",
            "validation_accuracy =  23.733333333333334\n",
            "2\n",
            "training loss =  0.4423784562936012\n",
            "validation loss =  0.45932435416856954\n",
            "trianing accuracy =  28.966666666666665\n",
            "validation_accuracy =  29.233333333333334\n",
            "3\n",
            "training loss =  0.41356503450469123\n",
            "validation loss =  0.4238020660545265\n",
            "trianing accuracy =  33.51111111111111\n",
            "validation_accuracy =  33.6\n",
            "4\n",
            "training loss =  0.39363564709926857\n",
            "validation loss =  0.40286761807868376\n",
            "trianing accuracy =  37.17037037037037\n",
            "validation_accuracy =  37.46666666666667\n",
            "5\n",
            "training loss =  0.37582012990673097\n",
            "validation loss =  0.3845512688220395\n",
            "trianing accuracy =  40.75555555555555\n",
            "validation_accuracy =  40.9\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7aef1a2015b44aea06eb83c3b3759c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.293103…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_accuracy</td><td>▁▃▅▇█</td></tr><tr><td>training_loss</td><td>█▄▃▂▁</td></tr><tr><td>validation cost</td><td>█▃▂▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_accuracy</td><td>40.75556</td></tr><tr><td>training_loss</td><td>0.37582</td></tr><tr><td>validation cost</td><td>0.38455</td></tr><tr><td>validation_accuracy</td><td>40.9</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">amber-sweep-6</strong> at: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/p9tubfcw' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/p9tubfcw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230317_064546-p9tubfcw/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ax176qod with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tL2_lamb: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_f: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_mode: xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230317_065016-ax176qod</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/ax176qod' target=\"_blank\">eager-sweep-7</a></strong> to <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/ax176qod' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/ax176qod</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr_0.1_ac_sigmoid_in_xavier_op_nadam_bs_16_L2_0_ep_5_nn_64_nh_2\n",
            "1\n",
            "training loss =  0.5114140553385886\n",
            "validation loss =  0.5128201794760844\n",
            "trianing accuracy =  10.005555555555556\n",
            "validation_accuracy =  9.95\n",
            "2\n",
            "training loss =  0.5131119290374182\n",
            "validation loss =  0.5427866052021344\n",
            "trianing accuracy =  10.044444444444444\n",
            "validation_accuracy =  9.6\n",
            "3\n",
            "training loss =  0.8927911040331274\n",
            "validation loss =  0.8961762617815516\n",
            "trianing accuracy =  10.035185185185185\n",
            "validation_accuracy =  9.683333333333334\n",
            "4\n",
            "training loss =  0.5113402739402172\n",
            "validation loss =  0.5298739747060351\n",
            "trianing accuracy =  9.975925925925926\n",
            "validation_accuracy =  10.216666666666667\n",
            "5\n",
            "training loss =  0.7207436635996881\n",
            "validation loss =  0.9052252892227872\n",
            "trianing accuracy =  9.898148148148149\n",
            "validation_accuracy =  10.916666666666666\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d71a7d4b838497bb1486ac0b57644c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_accuracy</td><td>▆██▅▁</td></tr><tr><td>training_loss</td><td>▁▁█▁▅</td></tr><tr><td>validation cost</td><td>▁▂█▁█</td></tr><tr><td>validation_accuracy</td><td>▃▁▁▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_accuracy</td><td>9.89815</td></tr><tr><td>training_loss</td><td>0.72074</td></tr><tr><td>validation cost</td><td>0.90523</td></tr><tr><td>validation_accuracy</td><td>10.91667</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">eager-sweep-7</strong> at: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/ax176qod' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/ax176qod</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230317_065016-ax176qod/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ppn3crdh with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tL2_lamb: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_f: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_mode: random_normal\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230317_065640-ppn3crdh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/ppn3crdh' target=\"_blank\">apricot-sweep-8</a></strong> to <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/ppn3crdh' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/ppn3crdh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr_0.01_ac_sigmoid_in_random_normal_op_adam_bs_32_L2_0_ep_5_nn_64_nh_3\n",
            "1\n",
            "training loss =  0.20294032691450825\n",
            "validation loss =  0.266924157541263\n",
            "trianing accuracy =  71.46296296296296\n",
            "validation_accuracy =  71.9\n",
            "2\n",
            "training loss =  0.21271970587756825\n",
            "validation loss =  0.2092393519493381\n",
            "trianing accuracy =  69.61666666666666\n",
            "validation_accuracy =  69.1\n",
            "3\n",
            "training loss =  0.21423292005437006\n",
            "validation loss =  0.20962414631690574\n",
            "trianing accuracy =  67.04629629629629\n",
            "validation_accuracy =  67.95\n",
            "4\n",
            "training loss =  0.2148328781770803\n",
            "validation loss =  0.21093007658233334\n",
            "trianing accuracy =  66.96481481481482\n",
            "validation_accuracy =  67.85\n",
            "5\n",
            "training loss =  0.21581673530657242\n",
            "validation loss =  0.21094035621177773\n",
            "trianing accuracy =  70.14444444444445\n",
            "validation_accuracy =  70.8\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35708804a04a458f9e1474425dffef7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.009 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.070589…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_accuracy</td><td>█▅▁▁▆</td></tr><tr><td>training_loss</td><td>▁▆▇▇█</td></tr><tr><td>validation cost</td><td>█▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>█▃▁▁▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_accuracy</td><td>70.14444</td></tr><tr><td>training_loss</td><td>0.21582</td></tr><tr><td>validation cost</td><td>0.21094</td></tr><tr><td>validation_accuracy</td><td>70.8</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">apricot-sweep-8</strong> at: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/ppn3crdh' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/ppn3crdh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230317_065640-ppn3crdh/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hra90z8z with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tL2_lamb: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_f: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_mode: xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230317_070334-hra90z8z</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/hra90z8z' target=\"_blank\">light-sweep-9</a></strong> to <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/hra90z8z' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/hra90z8z</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr_0.01_ac_tanh_in_xavier_op_adam_bs_32_L2_0.0005_ep_5_nn_128_nh_4\n",
            "1\n",
            "training loss =  0.4230454487010719\n",
            "validation loss =  0.28595977652140636\n",
            "trianing accuracy =  10.005555555555556\n",
            "validation_accuracy =  9.95\n",
            "2\n",
            "training loss =  0.4622705404020623\n",
            "validation loss =  0.46177257640771296\n",
            "trianing accuracy =  10.005555555555556\n",
            "validation_accuracy =  9.95\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "sweep_config = {\n",
        "  \"name\": \"CS6910 Assignment 1 - Squared Error Loss\",\n",
        "  \"metric\": {\n",
        "      \"name\":\"validation_accuracy\",\n",
        "      \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"method\": \"random\",\n",
        "  \"parameters\": {\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.1, 0.01]\n",
        "        },\n",
        "        \"activation_f\": {\n",
        "            \"values\": [\"sigmoid\", \"relu\", \"tanh\"]\n",
        "        },\n",
        "        \"init_mode\": {\n",
        "            \"values\": [\"xavier\", \"random_normal\"]\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"sgd\", \"momentum\", \"nag\", \"adam\", \"nadam\", \"rms\"]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [16,32,64]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [5]\n",
        "        },\n",
        "        \"L2_lamb\": {\n",
        "            \"values\": [0, 0.0005, 0.5]\n",
        "        },\n",
        "        \"num_neurons\": {\n",
        "            \"values\": [32,64,128]\n",
        "        },\n",
        "        \"num_hidden\": {\n",
        "            \"values\": [2,3,4]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"rajmahajan24\", project=\"DL_A1_optimizers_combined_13_01(mse)\")\n",
        "wandb.agent(sweep_id, run_NN, count=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuN06PEiCcJg"
      },
      "outputs": [],
      "source": [
        "# wandb.log({\"training_acc\": train_acc, \"validation_accuracy\": val_acc, \"training_loss\": cost, \"validation cost\": val_cost, 'epoch': count})\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
