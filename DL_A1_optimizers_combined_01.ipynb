{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "poDfvogCn0d-"
      },
      "outputs": [],
      "source": [
        "# Name - Raj Mahajan\n",
        "# Roll Number - CS22M067\n",
        "# Assignment 1\n",
        "# Question 3\n",
        "# Optimizers combined\n",
        "# adding mse\n",
        "# updating backward propogaion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login(key = \"c28eda24dacbd50a57332e445cfdfb2f4c16799c\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk8K1FuZUq-N",
        "outputId": "8d63ce6f-9010-4ea9-a0b2-e1ceb4304243"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.17.0-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.4.3)\n",
            "Collecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=19b9448e374ea3f613f30ee181bbe22bb050fdd2138dcc8a2a418c396bf87e75\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.17.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project=\"DL_A1_optimizers_combined_13.ipynb\")"
      ],
      "metadata": {
        "id": "j1QBwpKXU1_W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "fc2a8153-b32e-4ee1-fc0c-970e6797cfe3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajmahajan24\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230317_060859-gzqg21n6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb/runs/gzqg21n6' target=\"_blank\">magic-sunset-1</a></strong> to <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb/runs/gzqg21n6' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb/runs/gzqg21n6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13.ipynb/runs/gzqg21n6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7ff323e31280>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "xRTp4NxgpbsO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Fashion-MNIST dataset\n",
        "(X_train, Y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "X_train=X_train.reshape(X_train.shape[0],784)\n",
        "x_test=x_test.reshape(x_test.shape[0],784)\n",
        "Y_train=Y_train.reshape(60000,1)\n",
        "y_test=y_test.reshape(10000,1)\n",
        "# X_train.shape\n",
        "Y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBFE0h1Hp-k0",
        "outputId": "3e30d81f-d059-4ca4-d422-33b5792aad04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the data\n",
        "X_train=X_train/255.0\n",
        "x_test=x_test/255.0\n",
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgZvKQQ4qDJ-",
        "outputId": "ec9ecaa3-8062-4856-bb69-7b0cd86e2993"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_val,y_train,y_val = train_test_split(X_train , Y_train , test_size = 0.1 , random_state = 100)\n",
        "# x_train.shape\n",
        "x_val.shape"
      ],
      "metadata": {
        "id": "oPQgAvW_gO_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c0f45c-2879-4987-9d84-5d60696203ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def actual_prob_dist(y) :\n",
        "  ans=np.zeros((10, 1))\n",
        "  ans[y]=1\n",
        "  return(ans)\n",
        "\n",
        "def sigmoid(a) :\n",
        "  # s=1/(1+np.exp(a))\n",
        "\n",
        "  if isinstance(a,np.ndarray):\n",
        "    return 1.0 / (1.0 + np.exp(-a))\n",
        "  else :\n",
        "    if (x>=0):\n",
        "      return 1.0 / (1.0 + np.exp(-a))\n",
        "    else :\n",
        "      return np.exp(a) / ( 1.0 + np.exp(a))\n",
        "  # return(s)\n",
        "\n",
        "\n",
        "\n",
        "def derivative_sigmoid(x):\n",
        "  ans = sigmoid(x) * (1 -sigmoid(x))\n",
        "  return(ans)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def derivative_relu(x):\n",
        "    return 1*(x>0)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def derivative_tanh(x):\n",
        "    return (1 - (np.tanh(x)**2))\n",
        "\n",
        "def softmax(v):\n",
        "  # ans = np.exp(v) / np.sum(np.exp(v), axis=0)\n",
        "  z = v - max(v)\n",
        "  numerator = np.exp(z)\n",
        "  denominator = np.sum(numerator)\n",
        "  ans = numerator/denominator\n",
        "  return(ans)\n",
        "\n",
        "\n",
        "def derivative_softmax(x):\n",
        "  ans= softmax(x) * (1-softmax(x))\n",
        "  return(ans)\n",
        "\n",
        "\n",
        "def cross_entropy(y, yhat, batch_size):\n",
        "   loss = (-1.0 * np.sum(np.multiply(y, np.log(yhat))))/batch_size\n",
        "   return(loss)"
      ],
      "metadata": {
        "id": "EAuDaw6bo_Bu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_list_grad(list_grad_x , list_grad_y):\n",
        "  if(len(list_grad_x) == 0 ):\n",
        "    return(list_grad_y)\n",
        "  for i in range(0,len(list_grad_x)):\n",
        "    list_grad_x[i] += list_grad_y[i]\n",
        "  return(list_grad_x)\n",
        "\n",
        "def add_list_grad_eta(list_grad_x , list_grad_y, eta):\n",
        "  if(len(list_grad_x) == 0 ):\n",
        "    return(list_grad_y)\n",
        "  for i in range(0,len(list_grad_x)):\n",
        "    list_grad_x[i] -= eta * list_grad_y[i].transpose()\n",
        "  return(list_grad_x)\n",
        "\n",
        "def initilize_theta_random_normal(num_layers,neurons):\n",
        "  theta=[]\n",
        "  i=0\n",
        "  temp=np.random.randn(neurons[0], neurons[1])\n",
        "  theta.append(temp)\n",
        "  while i< len(neurons)-3 :\n",
        "    temp=np.random.randn(neurons[i+1], neurons[i+2])\n",
        "    theta.append(temp)\n",
        "    i+=1\n",
        "  temp=np.random.randn(neurons[-2], neurons[-1])\n",
        "  theta.append(temp)\n",
        "  return(theta)\n",
        "\n",
        "def initilize_theta_random_uniform(num_layers,neurons):\n",
        "  theta=[]\n",
        "  i=0\n",
        "  temp=np.random.rand(neurons[0], neurons[1])\n",
        "  theta.append(temp)\n",
        "  while i< len(neurons)-3 :\n",
        "    temp=np.random.rand(neurons[i+1], neurons[i+2])\n",
        "    theta.append(temp)\n",
        "    i+=1\n",
        "  temp=np.random.rand(neurons[-2], neurons[-1])\n",
        "  theta.append(temp)\n",
        "  return(theta)\n",
        "\n",
        "def initilize_theta_xavier(num_layers,neurons):\n",
        "  theta=[]\n",
        "  i=0\n",
        "  temp=np.random.randn(neurons[0], neurons[1]) * np.sqrt(2/(neurons[0]  + neurons[1])) # xavier\n",
        "  theta.append(temp)\n",
        "  while i< len(neurons)-3 :\n",
        "    temp=np.random.randn(neurons[i+1], neurons[i+2]) * np.sqrt(2/(neurons[i+1] + neurons[i+2])) # xavier\n",
        "    theta.append(temp)\n",
        "    i+=1\n",
        "  temp=np.random.randn(neurons[-2], neurons[-1]) * np.sqrt(2/(neurons[-2] + neurons[-1])) # xavier\n",
        "  theta.append(temp)\n",
        "  return(theta)\n",
        "\n",
        "def initilize_bias(num_layers,neurons):\n",
        "  b=[]\n",
        "  i=0\n",
        "  while i < num_layers :\n",
        "    temp=np.zeros((1,neurons[i+1]))\n",
        "    b.append(temp)\n",
        "    i+=1\n",
        "  temp=np.zeros((1, neurons[-1]))\n",
        "  b.append(temp)\n",
        "  return(b)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wQc_Udj0o_DW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def activation_fun(activation,a):\n",
        "  if(activation == \"sigmoid\"):\n",
        "    return(sigmoid(a))\n",
        "  elif(activation == \"relu\"):\n",
        "    return(relu(a))\n",
        "  elif(activation == \"tanh\"):\n",
        "    return(tanh(a))\n",
        "\n",
        "def derivative_activation_fun(activation,a):\n",
        "  if(activation == \"sigmoid\"):\n",
        "    return(derivative_sigmoid(a))\n",
        "  elif(activation == \"relu\"):\n",
        "    return(derivative_relu(a))\n",
        "  elif(activation == \"tanh\"):\n",
        "    return(derivative_tanh(a))\n",
        "  \n",
        "def loss_function(loss_fun,yhat,y):\n",
        "  if(loss_fun == \"cross_entropy\"):\n",
        "    loss = (-math.log(yhat[y]))\n",
        "  elif(loss_fun == \"mse\"):\n",
        "    Y = actual_prob_dist(y)\n",
        "    loss = (1/2) * np.sum((Y-yhat)**2)\n",
        "  return(loss)\n"
      ],
      "metadata": {
        "id": "UJCVfydl8XrV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constructed only for 1 image \n",
        "# h will be image input if 1d vector of length 28x28\n",
        "def forward_prop(activation,theta,b,num_layers,h):\n",
        "  h_list=[]\n",
        "  h_list.append(h)\n",
        "  a_list=[]\n",
        "  for k in range(0,num_layers):\n",
        "    a = b[k].transpose() + np.matmul(theta[k].transpose() , h)\n",
        "    a_list.append(a)\n",
        "    h= activation_fun(activation,a)\n",
        "    h_list.append(h)\n",
        "  # For last output layer\n",
        "  a = b[num_layers].transpose() + np.matmul(theta[num_layers].transpose() , h)\n",
        "  a_list.append(a)\n",
        "  yhat = softmax(a)\n",
        "  return(yhat,h_list,a_list)\n"
      ],
      "metadata": {
        "id": "rzS5d-JFo_E4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only for 1 image\n",
        "def backward_prop(theta,h_list,a_list,y,yhat,num_layers,batch_size,activation,loss_fun):\n",
        "  grad_a=-(actual_prob_dist(y) - yhat)\n",
        "  if(loss_fun == \"mse\"):\n",
        "      grad_a *= derivative_softmax(a_list[len(a_list)-1]);\n",
        "  list_grad_w=[]\n",
        "  list_grad_b=[]\n",
        "  for k in range(num_layers , -1,-1) :\n",
        "    grad_w = np.matmul(grad_a , h_list[k].transpose()) \n",
        "    list_grad_w.append(grad_w / batch_size)\n",
        "    grad_b = grad_a.copy()\n",
        "    list_grad_b.append(grad_b / batch_size)\n",
        "    if k >0:\n",
        "      grad_h = np.matmul(theta[k] , grad_a)\n",
        "      grad_a = grad_h * derivative_activation_fun(activation,a_list[k-1])\n",
        "  return(list_grad_w , list_grad_b)\n",
        "  \n",
        "\n",
        "# grad_b.shape = 32 x 1\n",
        "# grad_w.shape = 32 x 32\n",
        "# grad_a.shape = 10/32 x 1"
      ],
      "metadata": {
        "id": "b7FjUsDzo_Gc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_pridiction(x_test, y_test, theta, b, num_layers,activation):\n",
        "  img_no = 0\n",
        "  wrong_count = 0\n",
        "  while(img_no<len(x_test)):\n",
        "    img = x_test[img_no].reshape(784,1)\n",
        "    clas = y_test[img_no]\n",
        "    temp_ans = forward_prop(activation,theta,b,num_layers,img)\n",
        "    yhat = temp_ans[0]\n",
        "    temp = -1\n",
        "    predicted_class = -1\n",
        "    # print(yhat.shape)\n",
        "    sum=0\n",
        "    for i in range(0 , 10):\n",
        "      sum+=yhat[i]\n",
        "      if(temp < yhat[i]):\n",
        "        predicted_class = i\n",
        "        temp = yhat[i]\n",
        "    # print(sum)\n",
        "    # print(predicted_class)\n",
        "    if(clas != predicted_class):\n",
        "      wrong_count+=1\n",
        "    img_no+=1\n",
        "  accuracy = (len(x_test) - wrong_count) * 100 / len(x_test)\n",
        "  return(accuracy)"
      ],
      "metadata": {
        "id": "ugPI1ENFuZIi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SGD(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t<epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "         \n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      y_hat,h_list,a_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # training loss\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers,batch_size,activation,loss_fun)\n",
        "      list_grad_w = add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        theta = add_list_grad_eta(theta.copy() , list_grad_w, eta)\n",
        "        b = add_list_grad_eta(b.copy(), list_grad_b , eta)\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      # print(theta)\n",
        "      image_no += 1\n",
        "\n",
        "    # Update theta for last few images\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      theta = add_list_grad_eta(theta.copy() , list_grad_w, eta)\n",
        "      b = add_list_grad_eta(b.copy(), list_grad_b , eta)\n",
        "\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "\n",
        "  return(theta,b)"
      ],
      "metadata": {
        "id": "e3Jyo0kzr1jR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_SGD(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        "\n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    prev_grad_uw = [] # storing previous point\n",
        "    prev_grad_ub = [] # storing previous point\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers,batch_size,activation,loss_fun)\n",
        "      list_grad_w = add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "\n",
        "      if((image_no+1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        \n",
        "        if(len(prev_grad_uw) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            prev_grad_uw.append(beta * list_grad_w[i])\n",
        "            prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "          list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "        \n",
        "        theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "        b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "        \n",
        "        prev_grad_uw = list_u_grad_w.copy()\n",
        "        prev_grad_ub = list_u_grad_b.copy()\n",
        "        list_u_grad_w.clear()\n",
        "        list_u_grad_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    # Update theta for last few images\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      if(len(prev_grad_uw) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          prev_grad_uw.append(beta * list_grad_w[i])\n",
        "          prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "      for i in range(len(list_grad_w)):\n",
        "        list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "        list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "      \n",
        "      theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "      b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "      \n",
        "      prev_grad_uw = list_u_grad_w.copy()\n",
        "      prev_grad_ub = list_u_grad_b.copy()\n",
        "      list_u_grad_w.clear()\n",
        "      list_u_grad_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "  return(theta,b)"
      ],
      "metadata": {
        "id": "wyITICU2r1UN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RMS_SGD(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        " \n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    prev_grad_uw = [] # storing previous point\n",
        "    prev_grad_ub = [] # storing previous point\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train) \n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)\n",
        "      list_grad_w=add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b=add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "\n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        if(len(prev_grad_uw) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w.append((1 - beta) * list_grad_w[i] ** 2)\n",
        "            list_u_grad_b.append((1 - beta) * list_grad_b[i] ** 2)\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i] ** 2)\n",
        "            list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i] ** 2)\n",
        "        \n",
        "        for i in range(0,len(list_grad_w)):\n",
        "          theta[i] -= (eta * list_grad_w[i].transpose())/(list_u_grad_w[i].transpose() + 1e-6) ** 0.5\n",
        "          b[i] -= (eta * list_grad_b[i].transpose())/(list_u_grad_b[i].transpose()+ 1e-6) ** 0.5\n",
        "        \n",
        "        prev_grad_uw = list_u_grad_w.copy()\n",
        "        prev_grad_ub = list_u_grad_b.copy()\n",
        "        list_u_grad_w.clear()\n",
        "        list_u_grad_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      if(len(prev_grad_uw) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          prev_grad_uw.append(beta * list_grad_w[i])\n",
        "          prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "      for i in range(len(list_grad_w)):\n",
        "        list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i] ** 2)\n",
        "        list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i] ** 2)\n",
        "      \n",
        "      for i in range(0,len(list_grad_w)):\n",
        "        theta[i] -= (eta * list_grad_w[i].transpose())/(list_u_grad_w[i].transpose() + 1e-6) ** 0.5\n",
        "        b[i] -= (eta * list_grad_b[i].transpose())/(list_u_grad_b[i].transpose()+ 1e-6) ** 0.5\n",
        "      \n",
        "      prev_grad_uw = list_u_grad_w.copy()\n",
        "      prev_grad_ub = list_u_grad_b.copy()\n",
        "      list_u_grad_w.clear()\n",
        "      list_u_grad_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "  return(theta,b)\n"
      ],
      "metadata": {
        "id": "N7Gz1W23EO7U"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adam_SGD(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        "  \n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    list_m_w = [] # storing previous point\n",
        "    list_m_b = [] # storing previous point\n",
        "    list_v_w = [] # storing previous point\n",
        "    list_v_b = [] # storing previous point\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while( image_no < len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)\n",
        "      list_grad_w=add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b=add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      \n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        if(len(list_m_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "            list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "            list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "        \n",
        "        if(len(list_v_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "            list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "            list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "        \n",
        "        list_mhat_w = []\n",
        "        list_mhat_b = []\n",
        "        list_vhat_w = []\n",
        "        list_vhat_b = []\n",
        "        for i in range(len(list_m_w)):\n",
        "          list_mhat_w.append(list_m_w[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_mhat_b.append(list_m_b[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_w.append(list_v_w[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_b.append(list_v_b[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "\n",
        "        \n",
        "        # print(theta)\n",
        "        for i in range(0,len(list_grad_w)):\n",
        "          theta[i] -= (eta * list_mhat_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "          b[i] -= (eta * list_mhat_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "        \n",
        "        list_mhat_w.clear()\n",
        "        list_mhat_b.clear()\n",
        "        list_vhat_w.clear()\n",
        "        list_vhat_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      if(len(list_m_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "          list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "          list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "      \n",
        "      if(len(list_v_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "          list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "          list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "      \n",
        "      list_mhat_w = []\n",
        "      list_mhat_b = []\n",
        "      list_vhat_w = []\n",
        "      list_vhat_b = []\n",
        "      for i in range(len(list_m_w)):\n",
        "        list_mhat_w.append(list_m_w[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_mhat_b.append(list_m_b[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_vhat_w.append(list_v_w[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "        list_vhat_b.append(list_v_b[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "\n",
        "      \n",
        "      # print(theta)\n",
        "      for i in range(0,len(list_grad_w)):\n",
        "        theta[i] -= (eta * list_mhat_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "        b[i] -= (eta * list_mhat_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "      \n",
        "      list_mhat_w.clear()\n",
        "      list_mhat_b.clear()\n",
        "      list_vhat_w.clear()\n",
        "      list_vhat_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "  return(theta,b)\n"
      ],
      "metadata": {
        "id": "7d67UB_mGtGb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NAG(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        "  \n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    prev_grad_uw = [] # storing previous point\n",
        "    prev_grad_ub = [] # storing previous point\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      w_lookahead = []\n",
        "      b_lookahead = []\n",
        "      if(len(prev_grad_uw)==0):\n",
        "        w_lookahead = theta.copy()\n",
        "        b_lookahead = b.copy();\n",
        "      else :\n",
        "        for i in range(len(prev_grad_uw)):\n",
        "          w_lookahead.append(theta[i] - beta * prev_grad_uw[i].transpose())\n",
        "          b_lookahead.append(b[i] - beta * prev_grad_ub[i].transpose())\n",
        "      \n",
        "      fp_list = forward_prop(activation,w_lookahead,b_lookahead,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train) \n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "      bp_list = backward_prop(w_lookahead,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)\n",
        "      list_grad_w = add_list_grad(list_grad_w.copy() , bp_list[0])\n",
        "      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      w_lookahead.clear()\n",
        "      b_lookahead.clear()\n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        if(len(prev_grad_uw) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w = list_grad_w.copy()\n",
        "            list_u_grad_b = list_grad_b.copy()\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "            list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "        \n",
        "        # print(theta)\n",
        "        theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "        b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "        \n",
        "        prev_grad_uw = list_u_grad_w.copy()\n",
        "        prev_grad_ub = list_u_grad_b.copy()\n",
        "        list_u_grad_w.clear()\n",
        "        list_u_grad_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "        \n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      if(len(prev_grad_uw) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          prev_grad_uw.append(beta * list_grad_w[i])\n",
        "          prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "      for i in range(len(list_grad_w)):\n",
        "        list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "        list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "      \n",
        "      theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "      b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "      \n",
        "      prev_grad_uw = list_u_grad_w.copy()\n",
        "      prev_grad_ub = list_u_grad_b.copy()\n",
        "      list_u_grad_w.clear()\n",
        "      list_u_grad_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "  return(theta,b)\n"
      ],
      "metadata": {
        "id": "WZmDJQ4BfEpt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nadam(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):\n",
        "\n",
        "  t=0\n",
        "  global x_val\n",
        "  global y_val\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    list_m_w = [] # storing previous point\n",
        "    list_m_b = [] # storing previous point\n",
        "    list_v_w = [] # storing previous point\n",
        "    list_v_b = [] # storing previous point\n",
        "    loss = 0\n",
        "    val_loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "      # validation loss\n",
        "      if(image_no < len(x_val)):\n",
        "        h_v = x_val[image_no].reshape(784,1)\n",
        "        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)\n",
        "        y_v = y_val[image_no]\n",
        "        if(y_hat_v[y_v] != 0):\n",
        "          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)\n",
        "\n",
        "      # back propogation\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)\n",
        "      list_grad_w=add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b=add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      \n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "        if(len(list_m_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "            list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "            list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "        \n",
        "        if(len(list_v_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "            list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "            list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "        \n",
        "        list_mhat_w = []\n",
        "        list_mhat_b = []\n",
        "        list_vhat_w = []\n",
        "        list_vhat_b = []\n",
        "        for i in range(len(list_m_w)):\n",
        "          list_mhat_w.append(list_m_w[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_mhat_b.append(list_m_b[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_w.append(list_v_w[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_b.append(list_v_b[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "\n",
        "        \n",
        "        for i in range(0,len(list_grad_w)):\n",
        "          theta[i] -= (eta * list_mhat_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "          b[i] -= (eta * list_mhat_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "        \n",
        "        list_mhat_w.clear()\n",
        "        list_mhat_b.clear()\n",
        "        list_vhat_w.clear()\n",
        "        list_vhat_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      for j in range(len(list_grad_w)):\n",
        "          list_grad_w[j] += weight_decay * theta[j].transpose();\n",
        "      if(len(list_m_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "          list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "          list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "      \n",
        "      if(len(list_v_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "          list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "          list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "      \n",
        "      list_mhat_w = []\n",
        "      list_mhat_b = []\n",
        "      list_vhat_w = []\n",
        "      list_vhat_b = []\n",
        "      for i in range(len(list_m_w)):\n",
        "        list_mhat_w.append(list_m_w[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_mhat_b.append(list_m_b[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_vhat_w.append(list_v_w[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "        list_vhat_b.append(list_v_b[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "\n",
        "      nadam_term_w = []\n",
        "      nadam_term_b = []\n",
        "      for i in range(len(list_grad_w)):\n",
        "        nadam_term_w.append( beta1 * list_mhat_w[i] + ( (1 - beta1) * list_grad_w[i]) / (1 - beta1 ** int(image_no /batch_size) ))\n",
        "        nadam_term_b.append( beta1 * list_mhat_b[i] + ( (1 - beta1) * list_grad_b[i]) / (1 - beta1 ** int(image_no /batch_size) ))\n",
        "      # print(theta)\n",
        "      for i in range(0,len(list_grad_w)):\n",
        "        theta[i] -= (eta * nadam_term_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "        b[i] -= (eta * nadam_term_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "      \n",
        "      nadam_term_w.clear()\n",
        "      nadam_term_b.clear()\n",
        "      list_mhat_w.clear()\n",
        "      list_mhat_b.clear()\n",
        "      list_vhat_w.clear()\n",
        "      list_vhat_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "    for i in range(len(theta)):\n",
        "        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))\n",
        "        loss += temp / len(x_train)\n",
        "        val_loss += temp / len(x_val)\n",
        "    print(\"training loss = \",loss)\n",
        "    print(\"validation loss = \",val_loss)\n",
        "    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)\n",
        "    print(\"trianing accuracy = \",train_accuracy)\n",
        "    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(\"validation_accuracy = \",val_accuracy)\n",
        "    wandb.log({\"train_accuracy\": train_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": loss, \"validation cost\": val_loss, 'epoch': t})\n",
        "  return(theta,b)\n"
      ],
      "metadata": {
        "id": "H-M3MQXIuGbo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "\n",
        "# LEARNING_RATE = 0.001\n",
        "# ACTIVATION = \"sigmoid\"\n",
        "# INITIALIZER = \"xavier\"\n",
        "# OPTIMIZER = \"nag\"\n",
        "# BATCH_SIZE = 32\n",
        "# EPOCHS = 10\n",
        "# L2_lambda = 0.0005\n",
        "# LAYER_DIMS = [784,64,32,64,10]\n",
        "\n",
        "# LOSS = 'categorical_crossentropy'\n",
        "\n",
        "\n",
        "# learning_rate = 0.01\n",
        "# beta1 = 0.9\n",
        "# beta2 = 0.999\n",
        "# activation = \"relu\"\n",
        "# initializer = \"xavier\"\n",
        "# optimizer = \"momentum\"\n",
        "# batch_size = 32\n",
        "# epochs = 10\n",
        "# loss_fun = \"cross_entropy\"\n",
        "# neurons = [784,32,32,10]\n",
        "# num_layers = len(neurons) - 2\n",
        "# weight_decay = 0.0005"
      ],
      "metadata": {
        "id": "O1eNrF00vC9A"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network(x_train,y_train,x_test,y_test,eta,beta1,beta2,activation,initializer,optimizer,batch_size,epochs,loss_fun,weight_decay,num_layers,neurons):\n",
        "  neurons = [neurons]*(num_layers+2)\n",
        "  neurons[0] = 784\n",
        "  neurons[-1] = 10\n",
        "  if(initializer == \"xavier\"):\n",
        "    theta = initilize_theta_xavier(num_layers,neurons)\n",
        "    b = initilize_bias(num_layers,neurons)\n",
        "  elif (initializer == \"random_normal\"):\n",
        "    theta = initilize_theta_random_normal(num_layers,neurons)\n",
        "    b = initilize_bias(num_layers,neurons)\n",
        "  elif (initializer == \"random_uniform\"):\n",
        "    theta = initilize_theta_random_uniform(num_layers,neurons)\n",
        "    b = initilize_bias(num_layers,neurons)\n",
        "  else :\n",
        "    print(\"Invalid Intilizer\")\n",
        "  \n",
        "  # invoking training fun\n",
        "  if(optimizer == \"sgd\"):\n",
        "    theta,b = SGD(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  elif(optimizer == \"momentum\"):\n",
        "    theta,b = momentum_SGD(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  elif(optimizer == \"rms\"):\n",
        "    theta,b = RMS_SGD(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  elif(optimizer == \"adam\"):\n",
        "    theta,b = adam_SGD(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  elif(optimizer == \"nag\"):\n",
        "    theta,b = NAG(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  elif(optimizer == \"nadam\"):\n",
        "    theta,b = nadam(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)\n",
        "  else :\n",
        "    print(\"Invalid Optimizer\")\n",
        "\n",
        "\n",
        "  return(theta,b)\n",
        " \n"
      ],
      "metadata": {
        "id": "SWc5Sxa1o_J5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# theta , b =neural_network(x_train,y_train,x_test,y_test,learning_rate,beta1,beta2,activation,initializer,optimizer,batch_size,epochs,loss_fun,weight_decay)\n"
      ],
      "metadata": {
        "id": "qC8P9Y4RC73D"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  # testing accuracy\n",
        "# accuracy = image_pridiction(x_test, y_test, theta, b, num_layers,activation)\n",
        "# print(accuracy)"
      ],
      "metadata": {
        "id": "ncr6h7igElO1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NN_fit():\n",
        "    # Default values for hyper-parameters\n",
        "    # config_defaults = {\n",
        "    #     'epochs': 10,\n",
        "    #     'batch_size': 64,\n",
        "    #     'learning_rate': 1e-2,\n",
        "    #     'activation_f': 'relu',\n",
        "    #     'optimizer': 'adam',\n",
        "    #     'init_mode': 'xavier',\n",
        "    #     'L2_lamb': 0,\n",
        "    #     'num_neurons': 32,\n",
        "    #     'num_hidden': 2\n",
        "    # }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init()\n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "\n",
        "    # Local variables, values obtained from wandb config\n",
        "    num_neurons = config.num_neurons\n",
        "    num_hidden = config.num_hidden\n",
        "    init_mode = config.init_mode\n",
        "    epochs = config.epochs\n",
        "    batch_size = config.batch_size\n",
        "    learning_rate = config.learning_rate\n",
        "    activation_f = config.activation_f\n",
        "    L2_lamb = config.L2_lamb\n",
        "    optimizer = config.optimizer\n",
        "\n",
        "    # Display the hyperparameters\n",
        "    run_name = \"lr_{}_ac_{}_in_{}_op_{}_bs_{}_L2_{}_ep_{}_nn_{}_nh_{}\".format(learning_rate, activation_f, init_mode, optimizer, batch_size, L2_lamb, epochs, num_neurons, num_hidden)\n",
        "    print(run_name)\n",
        "    neural_network(x_train,y_train,x_test,y_test,learning_rate,0.9,0.999,activation_f,init_mode,optimizer,batch_size,epochs,\"mse\",0.0005,num_hidden,num_neurons)\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "-6qND3HuJGEd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "  \"name\": \"CS6910 Assignment 1 - Squared Error Loss\",\n",
        "  \"metric\": {\n",
        "      \"name\":\"validation_accuracy\",\n",
        "      \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"method\": \"random\",\n",
        "  \"parameters\": {\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.1, 0.01]\n",
        "        },\n",
        "        \"activation_f\": {\n",
        "            \"values\": [\"sigmoid\", \"relu\", \"tanh\"]\n",
        "        },\n",
        "        \"init_mode\": {\n",
        "            \"values\": [\"xavier\", \"random_normal\"]\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"sgd\", \"momentum\", \"nag\", \"adam\", \"nadam\", \"rms\"]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [16,32,64]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [5]\n",
        "        },\n",
        "        \"L2_lamb\": {\n",
        "            \"values\": [0, 0.0005, 0.5]\n",
        "        },\n",
        "        \"num_neurons\": {\n",
        "            \"values\": [32,64,128]\n",
        "        },\n",
        "        \"num_hidden\": {\n",
        "            \"values\": [2,3,4]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"rajmahajan24\", project=\"DL_A1_optimizers_combined_13_01(mse)\")\n",
        "wandb.agent(sweep_id, NN_fit, count=120)"
      ],
      "metadata": {
        "id": "2npDhTfaDH7V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9636330f-3e2f-4e9e-f311-ab11043b94af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: zs5wny9u\n",
            "Sweep URL: https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread NetStatThr:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.9/threading.py\", line 917, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py\", line 258, in check_network_status\n",
            "    self._loop_check_status(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py\", line 214, in _loop_check_status\n",
            "    local_handle = request()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/interface.py\", line 795, in deliver_network_status\n",
            "    return self._deliver_network_status(status)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/interface_shared.py\", line 601, in _deliver_network_status\n",
            "    return self._deliver_record(record)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/interface_shared.py\", line 560, in _deliver_record\n",
            "    handle = mailbox._deliver_record(record, interface=self)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
            "    interface._publish(record)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3w4icr3o with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tL2_lamb: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_f: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_mode: random_normal\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230317_060919-3w4icr3o</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/3w4icr3o' target=\"_blank\">stellar-sweep-1</a></strong> to <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/sweeps/zs5wny9u</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/3w4icr3o' target=\"_blank\">https://wandb.ai/rajmahajan24/DL_A1_optimizers_combined_13_01%28mse%29/runs/3w4icr3o</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr_0.01_ac_relu_in_random_normal_op_adam_bs_32_L2_0_ep_5_nn_64_nh_2\n",
            "1\n",
            "training loss =  0.2818688300925255\n",
            "validation loss =  0.4183552244436207\n",
            "trianing accuracy =  9.964814814814815\n",
            "validation_accuracy =  10.316666666666666\n",
            "2\n",
            "training loss =  0.5295248268806065\n",
            "validation loss =  0.7186153042152723\n",
            "trianing accuracy =  10.037037037037036\n",
            "validation_accuracy =  9.666666666666666\n",
            "3\n",
            "training loss =  0.6750144606105285\n",
            "validation loss =  0.7712773511636248\n",
            "trianing accuracy =  10.005555555555556\n",
            "validation_accuracy =  9.95\n",
            "4\n",
            "training loss =  0.7249056064466013\n",
            "validation loss =  0.7111098726904982\n",
            "trianing accuracy =  10.037037037037036\n",
            "validation_accuracy =  9.666666666666666\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.log({\"training_acc\": train_acc, \"validation_accuracy\": val_acc, \"training_loss\": cost, \"validation cost\": val_cost, 'epoch': count})\n"
      ],
      "metadata": {
        "id": "GuN06PEiCcJg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}