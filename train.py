# -*- coding: utf-8 -*-
"""DL_A1_optimizers_combined_14.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j4J_GMOUylow_tfqbVOtC7y8RlZqYODN
"""

# Name - Raj Mahajan
# Roll Number - CS22M067
# Assignment 1
# Neural Network

import wandb
wandb.login(key = "c28eda24dacbd50a57332e445cfdfb2f4c16799c")



import numpy as np
import math
import pandas as pd
import matplotlib.pyplot as plt

# taking parameters.
import argparse
parser = argparse.ArgumentParser()
parser.add_argument('-wp' , '--wandb_project' , required = False , metavar = "" , default = 'CS6910_A1_final' , type = str , help = "Project name used to track experiments in Weights & Biases dashboard")
parser.add_argument('-we' , '--wandb_entity' , required = False , metavar = "" , default = 'rajmahajan24' , type = str , help = "Wandb Entity used to track experiments in the Weights & Biases dashboard.")
parser.add_argument('-d' , '--dataset' , required = False , metavar = "" , default = 'fashion_mnist' , type = str , choices = ["mnist","fashion_mnist"] ,help = 'choices: ["mnist", "fashion_mnist"]')
parser.add_argument('-e' , '--epochs' , required = False , metavar = "" , default = '10' , type = str , help = "Number of epochs to train neural network.")
parser.add_argument('-b' , '--batch_size' , required = False , metavar = "" , default = '64' , type = str , help = "Batch size used to train neural network.")
parser.add_argument('-l' , '--loss' , required = False , metavar = "" , default = 'cross_entropy' , type = str , choices = ["mean_squared_error", "cross_entropy"] , help = 'choices: ["mean_squared_error", "cross_entropy"]')
parser.add_argument('-o' , '--optimizer' , required = False , metavar = "" , default = 'nag' , type = str , choices = ["sgd", "momentum", "nag", "rmsprop", "adam", "nadam"] , help = 'choices: ["sgd", "momentum", "nag", "rmsprop", "adam", "nadam"]')
parser.add_argument('-lr' , '--learning_rate' , required = False , metavar = "" , default = '0.1' , type = str , help = "Learning rate used to optimize model parameters")
parser.add_argument('-m' , '--momentum' , required = False , metavar = "" , default = '0.9' , type = str , help = "Momentum used by momentum and nag optimizers.")
parser.add_argument('-beta' , '--beta' , required = False , metavar = "" , default = '0.5' , type = str , help = "Beta used by rmsprop optimizer")
parser.add_argument('-beta1' , '--beta1' , required = False , metavar = "" , default = '0.5' , type = str , help = "Beta1 used by adam and nadam optimizers.")
parser.add_argument('-beta2' , '--beta2' , required = False , metavar = "" , default = '0.5' , type = str , help = "Beta2 used by adam and nadam optimizers.")
parser.add_argument('-eps' , '--epsilon' , required = False , metavar = "" , default = '0.000001' , type = str , help = "Epsilon used by optimizers.")
parser.add_argument('-w_d' , '--weight_decay' , required = False , metavar = "" , default = '0.0005' , type = str , help = "Weight decay used by optimizers.")
parser.add_argument('-w_i' , '--weight_init' , required = False , metavar = "" , default = 'Xavier' , type = str , choices = ["random", "Xavier"] , help = 'choices: ["random", "Xavier"]')
parser.add_argument('-nhl' , '--num_layers' , required = False , metavar = "" , default = '2' , type = str , help = "Number of hidden layers used in feedforward neural network.")
parser.add_argument('-sz' , '--hidden_size' , required = False , metavar = "" , default = '128' , type = str , help = "Number of hidden neurons in a feedforward layer.")
parser.add_argument('-a' , '--activation' , required = False , metavar = "" , default = 'tanh' , type = str , choices = ["identity", "sigmoid", "tanh", "ReLU"] , help = 'choices: ["identity", "sigmoid", "tanh", "ReLU"]')
argparse = parser.parse_args()
if(argparse.dataset == 'fashion_mnist'):
    from keras.datasets import fashion_mnist
    (X_train, Y_train), (x_test, y_test) = fashion_mnist.load_data()
elif(argparse.dataset == 'mnist'):
    from keras.datasets import mnist
    (X_train, Y_train), (x_test, y_test) = mnist.load_data()

from sklearn.model_selection import train_test_split

# reshaping the data
X_train=X_train.reshape(X_train.shape[0],784)
x_test=x_test.reshape(x_test.shape[0],784)
Y_train=Y_train.reshape(60000,1)
y_test=y_test.reshape(10000,1)

# Normalizing the data
X_train=X_train/255.0
x_test=x_test/255.0
y_test.shape

# splitting into train and validation set
x_train,x_val,y_train,y_val = train_test_split(X_train , Y_train , test_size = 0.1 , random_state = 100)

# creating wandb project
import wandb
wandb.init(project=argparse.wandb_project)

def actual_prob_dist(y) :
  ans=np.zeros((10, 1))
  ans[y]=1
  return(ans)

def sigmoid(a) :

  if isinstance(a,np.ndarray):
    return 1.0 / (1.0 + np.exp(-a))
  else :
    if (x>=0):
      return 1.0 / (1.0 + np.exp(-a))
    else :
      return np.exp(a) / ( 1.0 + np.exp(a))
  

def identity_fun(a) :
    return(a)
def derivative_identity(a) :
    return(np.ones(a.shape))

def derivative_sigmoid(x):
  ans = sigmoid(x) * (1 -sigmoid(x))
  return(ans)

def relu(x):
    return np.maximum(0,x)

def derivative_relu(x):
    return 1*(x>0)

def tanh(x):
    return np.tanh(x)

def derivative_tanh(x):
    return (1 - (np.tanh(x)**2))

def softmax(v):
  z = v - max(v)
  numerator = np.exp(z)
  denominator = np.sum(numerator)
  ans = numerator/denominator
  return(ans)


def derivative_softmax(x):
  ans= softmax(x) * (1-softmax(x))
  return(ans)


def cross_entropy(y, yhat, batch_size):
   loss = (-1.0 * np.sum(np.multiply(y, np.log(yhat))))/batch_size
   return(loss)

def add_list_grad(list_grad_x , list_grad_y):
  if(len(list_grad_x) == 0 ):
    return(list_grad_y)
  for i in range(0,len(list_grad_x)):
    list_grad_x[i] += list_grad_y[i]
  return(list_grad_x)

def add_list_grad_eta(list_grad_x , list_grad_y, eta):
  if(len(list_grad_x) == 0 ):
    return(list_grad_y)
  for i in range(0,len(list_grad_x)):
    list_grad_x[i] -= eta * list_grad_y[i].transpose()
  return(list_grad_x)

def initilize_theta_random_normal(num_layers,neurons):
  theta=[]
  i=0
  temp=np.random.randn(neurons[0], neurons[1])
  theta.append(temp)
  while i< len(neurons)-3 :
    temp=np.random.randn(neurons[i+1], neurons[i+2])
    theta.append(temp)
    i+=1
  temp=np.random.randn(neurons[-2], neurons[-1])
  theta.append(temp)
  return(theta)

def initilize_theta_random_uniform(num_layers,neurons):
  theta=[]
  i=0
  temp=np.random.rand(neurons[0], neurons[1])
  theta.append(temp)
  while i< len(neurons)-3 :
    temp=np.random.rand(neurons[i+1], neurons[i+2])
    theta.append(temp)
    i+=1
  temp=np.random.rand(neurons[-2], neurons[-1])
  theta.append(temp)
  return(theta)

def initilize_theta_xavier(num_layers,neurons):
  theta=[]
  i=0
  temp=np.random.randn(neurons[0], neurons[1]) * np.sqrt(2/(neurons[0]  + neurons[1])) # xavier
  theta.append(temp)
  while i< len(neurons)-3 :
    temp=np.random.randn(neurons[i+1], neurons[i+2]) * np.sqrt(2/(neurons[i+1] + neurons[i+2])) # xavier
    theta.append(temp)
    i+=1
  temp=np.random.randn(neurons[-2], neurons[-1]) * np.sqrt(2/(neurons[-2] + neurons[-1])) # xavier
  theta.append(temp)
  return(theta)

def initilize_bias(num_layers,neurons):
  b=[]
  i=0
  while i < num_layers :
    temp=np.zeros((1,neurons[i+1]))
    b.append(temp)
    i+=1
  temp=np.zeros((1, neurons[-1]))
  b.append(temp)
  return(b)

def activation_fun(activation,a):
  if(activation == "sigmoid"):
    return(sigmoid(a))
  elif(activation == "ReLU"):
    return(relu(a))
  elif(activation == "tanh"):
    return(tanh(a))
  elif(activation == "identity"):
    return(identity_fun(a))


def derivative_activation_fun(activation,a):
  if(activation == "sigmoid"):
    return(derivative_sigmoid(a))
  elif(activation == "ReLU"):
    return(derivative_relu(a))
  elif(activation == "tanh"):
    return(derivative_tanh(a))
  elif(activation == "identity"):
    return(derivative_identity(a))
  
def loss_function(loss_fun,yhat,y):
  if(loss_fun == "cross_entropy"):
    loss = (-math.log(yhat[y]))
  elif(loss_fun == "mean_squared_error"):
    Y = actual_prob_dist(y)
    loss = (1/2) * np.sum((Y-yhat)**2)
  return(loss)

# Constructed only for 1 image 
# h will be image input if 1d vector of length 28x28
def forward_prop(activation,theta,b,num_layers,h):
  h_list=[]
  h_list.append(h)
  a_list=[]
  for k in range(0,num_layers):
    a = b[k].transpose() + np.matmul(theta[k].transpose() , h)
    a_list.append(a)
    h= activation_fun(activation,a)
    h_list.append(h)
  # For last output layer
  a = b[num_layers].transpose() + np.matmul(theta[num_layers].transpose() , h)
  a_list.append(a)
  yhat = softmax(a)
  return(yhat,h_list,a_list)

# only for 1 imabackge
def backward_prop(theta,h_list,a_list,y,yhat,num_layers,batch_size,activation,loss_fun):
  grad_a=-(actual_prob_dist(y) - yhat)
  if(loss_fun == "mean_squared_error"):
      grad_a *= derivative_softmax(a_list[len(a_list)-1]);
  list_grad_w=[]
  list_grad_b=[]
  for k in range(num_layers , -1,-1) :
    grad_w = np.matmul(grad_a , h_list[k].transpose()) 
    list_grad_w.append(grad_w / batch_size)
    grad_b = grad_a.copy()
    list_grad_b.append(grad_b / batch_size)
    if k >0:
      grad_h = np.matmul(theta[k] , grad_a)
      grad_a = grad_h * derivative_activation_fun(activation,a_list[k-1])
  return(list_grad_w , list_grad_b)
  
# computes accuracy for given test data.
def image_pridiction(x_test, y_test, theta, b, num_layers,activation):
  img_no = 0
  wrong_count = 0
  while(img_no<len(x_test)):
    img = x_test[img_no].reshape(784,1)
    clas = y_test[img_no]
    temp_ans = forward_prop(activation,theta,b,num_layers,img)
    yhat = temp_ans[0]
    temp = -1
    predicted_class = -1
    sum=0
    for i in range(0 , 10):
      sum+=yhat[i]
      if(temp < yhat[i]):
        predicted_class = i
        temp = yhat[i]
    if(clas != predicted_class):
      wrong_count+=1
    img_no+=1
  accuracy = (len(x_test) - wrong_count) * 100 / len(x_test)
  return(accuracy)

def SGD(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):
  t=0
  global x_val
  global y_val
  while t<epochs :
    t += 1
    print(t)
    image_no = 0
    list_grad_w = []
    list_grad_b = []
    loss = 0
    val_loss = 0
    while(image_no<len(x_train)):
         
      # forward propgation
      h=x_train[image_no].reshape(784,1)
      y_hat,h_list,a_list = forward_prop(activation,theta,b,num_layers,h)
      # training loss
      y = y_train[image_no]
      if( y_hat[y]!= 0):
        loss += loss_function(loss_fun,y_hat,y) / len(x_train)
      # validation loss
      if(image_no < len(x_val)):
        h_v = x_val[image_no].reshape(784,1)
        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)
        y_v = y_val[image_no]
        if(y_hat_v[y_v] != 0):
          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)

      # back propogation
      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers,batch_size,activation,loss_fun)
      list_grad_w = add_list_grad(list_grad_w.copy(), bp_list[0])
      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])
      if((image_no + 1) % batch_size == 0):
        list_grad_w.reverse()
        list_grad_b.reverse()
        for j in range(len(list_grad_w)):
          list_grad_w[j] += weight_decay * theta[j].transpose();
        theta = add_list_grad_eta(theta.copy() , list_grad_w, eta)
        b = add_list_grad_eta(b.copy(), list_grad_b , eta)
        list_grad_w.clear()
        list_grad_b.clear()
      image_no += 1

    # Update theta for last few images
    if (len(x_train) % batch_size != 0):
      list_grad_w.reverse()
      list_grad_b.reverse()
      for j in range(len(list_grad_w)):
          list_grad_w[j] += weight_decay * theta[j].transpose();
      theta = add_list_grad_eta(theta.copy() , list_grad_w, eta)
      b = add_list_grad_eta(b.copy(), list_grad_b , eta)

    for i in range(len(theta)):
        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))
        loss += temp / len(x_train)
        val_loss += temp / len(x_val)
    print("training loss = ",loss)
    print("validation loss = ",val_loss)
    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)
    print("trianing accuracy = ",train_accuracy)
    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)
    print("validation_accuracy = ",val_accuracy)
    wandb.log({"train_accuracy": train_accuracy, "validation_accuracy": val_accuracy, "training_loss": loss, "validation cost": val_loss, 'epoch': t})

  return(theta,b)

def momentum_SGD(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):

  t=0
  global x_val
  global y_val
  while t < epochs :
    t += 1
    print(t)
    image_no = 0
    list_u_grad_w = []
    list_u_grad_b = []
    list_grad_w = []
    list_grad_b = []
    prev_grad_uw = [] # storing previous point
    prev_grad_ub = [] # storing previous point
    loss = 0
    val_loss = 0
    while(image_no<len(x_train)):
      # forward propgation
      h=x_train[image_no].reshape(784,1)
      fp_list = forward_prop(activation,theta,b,num_layers,h)
      # backward propogation
      a_list = fp_list[2]
      h_list = fp_list[1]
      y_hat = fp_list[0]
      y = y_train[image_no]
      if( y_hat[y]!= 0):
        loss += loss_function(loss_fun,y_hat,y) / len(x_train)
      # validation loss
      if(image_no < len(x_val)):
        h_v = x_val[image_no].reshape(784,1)
        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)
        y_v = y_val[image_no]
        if(y_hat_v[y_v] != 0):
          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)

      # back propogation

      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers,batch_size,activation,loss_fun)
      list_grad_w = add_list_grad(list_grad_w.copy(), bp_list[0])
      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])

      if((image_no+1) % batch_size == 0):
        list_grad_w.reverse()
        list_grad_b.reverse()
        for j in range(len(list_grad_w)):
          list_grad_w[j] += weight_decay * theta[j].transpose();
        
        if(len(prev_grad_uw) == 0):
          for i in range(len(list_grad_w)):
            prev_grad_uw.append(beta * list_grad_w[i])
            prev_grad_ub.append(beta * list_grad_b[i])

        for i in range(len(list_grad_w)):
          list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])
          list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])
        
        theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)
        b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)
        
        prev_grad_uw = list_u_grad_w.copy()
        prev_grad_ub = list_u_grad_b.copy()
        list_u_grad_w.clear()
        list_u_grad_b.clear()
        list_grad_w.clear()
        list_grad_b.clear()
      image_no += 1

    # Update theta for last few images
    if (len(x_train) % batch_size != 0):
      list_grad_w.reverse()
      list_grad_b.reverse()
      for j in range(len(list_grad_w)):
          list_grad_w[j] += weight_decay * theta[j].transpose();
      if(len(prev_grad_uw) == 0):
        for i in range(len(list_grad_w)):
          prev_grad_uw.append(beta * list_grad_w[i])
          prev_grad_ub.append(beta * list_grad_b[i])

      for i in range(len(list_grad_w)):
        list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])
        list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])
      
      theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)
      b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)
      
      prev_grad_uw = list_u_grad_w.copy()
      prev_grad_ub = list_u_grad_b.copy()
      list_u_grad_w.clear()
      list_u_grad_b.clear()
      list_grad_w.clear()
      list_grad_b.clear()

    for i in range(len(theta)):
        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))
        loss += temp / len(x_train)
        val_loss += temp / len(x_val)
    print("training loss = ",loss)
    print("validation loss = ",val_loss)
    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)
    print("trianing accuracy = ",train_accuracy)
    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)
    print("validation_accuracy = ",val_accuracy)
    wandb.log({"train_accuracy": train_accuracy, "validation_accuracy": val_accuracy, "training_loss": loss, "validation cost": val_loss, 'epoch': t})
  return(theta,b)

def RMS_SGD(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay,epsilon):
 
  t=0
  global x_val
  global y_val
  while t < epochs :
    t += 1
    print(t)
    image_no = 0
    list_u_grad_w = []
    list_u_grad_b = []
    list_grad_w = []
    list_grad_b = []
    prev_grad_uw = [] # storing previous point
    prev_grad_ub = [] # storing previous point
    loss = 0
    val_loss = 0
    while(image_no<len(x_train)):
      # forward propgation
      h=x_train[image_no].reshape(784,1)
      fp_list = forward_prop(activation,theta,b,num_layers,h)
      # backward propogation
      a_list = fp_list[2]
      h_list = fp_list[1]
      y_hat = fp_list[0]
      y = y_train[image_no]
      if( y_hat[y]!= 0):
        loss += loss_function(loss_fun,y_hat,y) / len(x_train) 
      # validation loss
      if(image_no < len(x_val)):
        h_v = x_val[image_no].reshape(784,1)
        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)
        y_v = y_val[image_no]
        if(y_hat_v[y_v] != 0):
          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)

      # back propogation
      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)
      list_grad_w=add_list_grad(list_grad_w.copy(), bp_list[0])
      list_grad_b=add_list_grad(list_grad_b.copy() , bp_list[1])

      if((image_no + 1) % batch_size == 0):
        list_grad_w.reverse()
        list_grad_b.reverse()
        for j in range(len(list_grad_w)):
          list_grad_w[j] += weight_decay * theta[j].transpose();
        if(len(prev_grad_uw) == 0):
          for i in range(len(list_grad_w)):
            list_u_grad_w.append((1 - beta) * list_grad_w[i] ** 2)
            list_u_grad_b.append((1 - beta) * list_grad_b[i] ** 2)
        else :
          for i in range(len(list_grad_w)):
            list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i] ** 2)
            list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i] ** 2)
        
        for i in range(0,len(list_grad_w)):
          theta[i] -= (eta * list_grad_w[i].transpose())/(list_u_grad_w[i].transpose() + epsilon) ** 0.5
          b[i] -= (eta * list_grad_b[i].transpose())/(list_u_grad_b[i].transpose()+ epsilon) ** 0.5
        
        prev_grad_uw = list_u_grad_w.copy()
        prev_grad_ub = list_u_grad_b.copy()
        list_u_grad_w.clear()
        list_u_grad_b.clear()
        list_grad_w.clear()
        list_grad_b.clear()
      image_no += 1

    if (len(x_train) % batch_size != 0):
      list_grad_w.reverse()
      list_grad_b.reverse()
      for j in range(len(list_grad_w)):
          list_grad_w[j] += weight_decay * theta[j].transpose();
      if(len(prev_grad_uw) == 0):
        for i in range(len(list_grad_w)):
          prev_grad_uw.append(beta * list_grad_w[i])
          prev_grad_ub.append(beta * list_grad_b[i])

      for i in range(len(list_grad_w)):
        list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i] ** 2)
        list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i] ** 2)
      
      for i in range(0,len(list_grad_w)):
        theta[i] -= (eta * list_grad_w[i].transpose())/(list_u_grad_w[i].transpose() + epsilon) ** 0.5
        b[i] -= (eta * list_grad_b[i].transpose())/(list_u_grad_b[i].transpose()+ epsilon) ** 0.5
      
      prev_grad_uw = list_u_grad_w.copy()
      prev_grad_ub = list_u_grad_b.copy()
      list_u_grad_w.clear()
      list_u_grad_b.clear()
      list_grad_w.clear()
      list_grad_b.clear()

    for i in range(len(theta)):
        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))
        loss += temp / len(x_train)
        val_loss += temp / len(x_val)
    print("training loss = ",loss)
    print("validation loss = ",val_loss)
    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)
    print("trianing accuracy = ",train_accuracy)
    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)
    print("validation_accuracy = ",val_accuracy)
    wandb.log({"train_accuracy": train_accuracy, "validation_accuracy": val_accuracy, "training_loss": loss, "validation cost": val_loss, 'epoch': t})
  return(theta,b)

def adam_SGD(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay,epsilon):
  
  t=0
  global x_val
  global y_val
  while t < epochs :
    t += 1
    print(t)
    image_no = 0
    list_u_grad_w = []
    list_u_grad_b = []
    list_grad_w = []
    list_grad_b = []
    list_m_w = [] # storing previous point
    list_m_b = [] # storing previous point
    list_v_w = [] # storing previous point
    list_v_b = [] # storing previous point
    loss = 0
    val_loss = 0
    while( image_no < len(x_train)):
      # forward propgation
      h=x_train[image_no].reshape(784,1)
      fp_list = forward_prop(activation,theta,b,num_layers,h)
      # backward propogation
      a_list = fp_list[2]
      h_list = fp_list[1]
      y_hat = fp_list[0]
      y = y_train[image_no]
      if( y_hat[y]!= 0):
        loss += loss_function(loss_fun,y_hat,y) / len(x_train)
      # validation loss
      if(image_no < len(x_val)):
        h_v = x_val[image_no].reshape(784,1)
        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)
        y_v = y_val[image_no]
        if(y_hat_v[y_v] != 0):
          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)

      # back propogation
      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)
      list_grad_w=add_list_grad(list_grad_w.copy(), bp_list[0])
      list_grad_b=add_list_grad(list_grad_b.copy() , bp_list[1])
      
      if((image_no + 1) % batch_size == 0):
        list_grad_w.reverse()
        list_grad_b.reverse()
        for j in range(len(list_grad_w)):
          list_grad_w[j] += weight_decay * theta[j].transpose();
        if(len(list_m_w) == 0):
          for i in range(len(list_grad_w)):
            list_m_w.append((1 - beta1) * list_grad_w[i] )
            list_m_b.append((1 - beta1) * list_grad_b[i] )
        else :
          for i in range(len(list_grad_w)):
            list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] 
            list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] 
        
        if(len(list_v_w) == 0):
          for i in range(len(list_grad_w)):
            list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )
            list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )
        else :
          for i in range(len(list_grad_w)):
            list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2
            list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2
        
        list_mhat_w = []
        list_mhat_b = []
        list_vhat_w = []
        list_vhat_b = []
        for i in range(len(list_m_w)):
          list_mhat_w.append(list_m_w[i]/(1-beta1 ** (int(image_no /batch_size)+1)))
          list_mhat_b.append(list_m_b[i]/(1-beta1 ** (int(image_no /batch_size)+1)))
          list_vhat_w.append(list_v_w[i]/(1-beta2 ** (int(image_no /batch_size)+1)))
          list_vhat_b.append(list_v_b[i]/(1-beta2 ** (int(image_no /batch_size)+1)))

        
        # print(theta)
        for i in range(0,len(list_grad_w)):
          theta[i] -= (eta * list_mhat_w[i].transpose())/(list_vhat_w[i].transpose() + epsilon) ** 0.5
          b[i] -= (eta * list_mhat_b[i].transpose())/(list_vhat_b[i].transpose()+ epsilon) ** 0.5
        
        list_mhat_w.clear()
        list_mhat_b.clear()
        list_vhat_w.clear()
        list_vhat_b.clear()
        list_grad_w.clear()
        list_grad_b.clear()
      image_no += 1

    if (len(x_train) % batch_size != 0):
      list_grad_w.reverse()
      list_grad_b.reverse()
      for j in range(len(list_grad_w)):
          list_grad_w[j] += weight_decay * theta[j].transpose();
      if(len(list_m_w) == 0):
        for i in range(len(list_grad_w)):
          list_m_w.append((1 - beta1) * list_grad_w[i] )
          list_m_b.append((1 - beta1) * list_grad_b[i] )
      else :
        for i in range(len(list_grad_w)):
          list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] 
          list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] 
      
      if(len(list_v_w) == 0):
        for i in range(len(list_grad_w)):
          list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )
          list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )
      else :
        for i in range(len(list_grad_w)):
          list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2
          list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2
      
      list_mhat_w = []
      list_mhat_b = []
      list_vhat_w = []
      list_vhat_b = []
      for i in range(len(list_m_w)):
        list_mhat_w.append(list_m_w[i]/(1-beta1 ** int(image_no /batch_size)))
        list_mhat_b.append(list_m_b[i]/(1-beta1 ** int(image_no /batch_size)))
        list_vhat_w.append(list_v_w[i]/(1-beta2 ** int(image_no /batch_size)))
        list_vhat_b.append(list_v_b[i]/(1-beta2 ** int(image_no /batch_size)))

      for i in range(0,len(list_grad_w)):
        theta[i] -= (eta * list_mhat_w[i].transpose())/(list_vhat_w[i].transpose() + epsilon) ** 0.5
        b[i] -= (eta * list_mhat_b[i].transpose())/(list_vhat_b[i].transpose()+ epsilon) ** 0.5
      
      list_mhat_w.clear()
      list_mhat_b.clear()
      list_vhat_w.clear()
      list_vhat_b.clear()
      list_grad_w.clear()
      list_grad_b.clear()
    for i in range(len(theta)):
        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))
        loss += temp / len(x_train)
        val_loss += temp / len(x_val)
    print("training loss = ",loss)
    print("validation loss = ",val_loss)
    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)
    print("trianing accuracy = ",train_accuracy)
    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)
    print("validation_accuracy = ",val_accuracy)
    wandb.log({"train_accuracy": train_accuracy, "validation_accuracy": val_accuracy, "training_loss": loss, "validation cost": val_loss, 'epoch': t})
  return(theta,b)

def NAG(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay):
  
  t=0
  global x_val
  global y_val
  while t < epochs :
    t += 1
    print(t)
    image_no = 0
    list_u_grad_w = []
    list_u_grad_b = []
    list_grad_w = []
    list_grad_b = []
    prev_grad_uw = [] # storing previous point
    prev_grad_ub = [] # storing previous point
    loss = 0
    val_loss = 0
    while(image_no<len(x_train)):
      
      h=x_train[image_no].reshape(784,1)
      w_lookahead = []
      b_lookahead = []
      if(len(prev_grad_uw)==0):
        w_lookahead = theta.copy()
        b_lookahead = b.copy();
      else :
        for i in range(len(prev_grad_uw)):
          w_lookahead.append(theta[i] - beta * prev_grad_uw[i].transpose())
          b_lookahead.append(b[i] - beta * prev_grad_ub[i].transpose())
      # forward propgation
      fp_list = forward_prop(activation,w_lookahead,b_lookahead,num_layers,h)
      # backward propogation
      a_list = fp_list[2]
      h_list = fp_list[1]
      y_hat = fp_list[0]
      y = y_train[image_no]
      if( y_hat[y]!= 0):
        loss += loss_function(loss_fun,y_hat,y) / len(x_train) 
      # validation loss
      if(image_no < len(x_val)):
        h_v = x_val[image_no].reshape(784,1)
        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)
        y_v = y_val[image_no]
        if(y_hat_v[y_v] != 0):
          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)

      # back propogation
      bp_list = backward_prop(w_lookahead,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)
      list_grad_w = add_list_grad(list_grad_w.copy() , bp_list[0])
      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])
      w_lookahead.clear()
      b_lookahead.clear()
      if((image_no + 1) % batch_size == 0):
        list_grad_w.reverse()
        list_grad_b.reverse()
        for j in range(len(list_grad_w)):
          list_grad_w[j] += weight_decay * theta[j].transpose();
        if(len(prev_grad_uw) == 0):
          for i in range(len(list_grad_w)):
            list_u_grad_w = list_grad_w.copy()
            list_u_grad_b = list_grad_b.copy()
        else :
          for i in range(len(list_grad_w)):
            list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])
            list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])
        
        theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)
        b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)
        
        prev_grad_uw = list_u_grad_w.copy()
        prev_grad_ub = list_u_grad_b.copy()
        list_u_grad_w.clear()
        list_u_grad_b.clear()
        list_grad_w.clear()
        list_grad_b.clear()
        
      image_no += 1

    if (len(x_train) % batch_size != 0):
      list_grad_w.reverse()
      list_grad_b.reverse()
      for j in range(len(list_grad_w)):
          list_grad_w[j] += weight_decay * theta[j].transpose();
      if(len(prev_grad_uw) == 0):
        for i in range(len(list_grad_w)):
          prev_grad_uw.append(beta * list_grad_w[i])
          prev_grad_ub.append(beta * list_grad_b[i])

      for i in range(len(list_grad_w)):
        list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])
        list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])
      
      theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)
      b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)
      
      prev_grad_uw = list_u_grad_w.copy()
      prev_grad_ub = list_u_grad_b.copy()
      list_u_grad_w.clear()
      list_u_grad_b.clear()
      list_grad_w.clear()
      list_grad_b.clear()
    for i in range(len(theta)):
        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))
        loss += temp / len(x_train)
        val_loss += temp / len(x_val)
    print("training loss = ",loss)
    print("validation loss = ",val_loss)
    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)
    print("trianing accuracy = ",train_accuracy)
    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)
    print("validation_accuracy = ",val_accuracy)
    wandb.log({"train_accuracy": train_accuracy, "validation_accuracy": val_accuracy, "training_loss": loss, "validation cost": val_loss, 'epoch': t})
  return(theta,b)

def nadam(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay,epsilon):

  t=0
  global x_val
  global y_val
  while t < epochs :
    t += 1
    print(t)
    image_no = 0
    list_u_grad_w = []
    list_u_grad_b = []
    list_grad_w = []
    list_grad_b = []
    list_m_w = [] # storing previous point
    list_m_b = [] # storing previous point
    list_v_w = [] # storing previous point
    list_v_b = [] # storing previous point
    loss = 0
    val_loss = 0
    while(image_no<len(x_train)):
      # forward propgation
      h=x_train[image_no].reshape(784,1)
      fp_list = forward_prop(activation,theta,b,num_layers,h)

      a_list = fp_list[2]
      h_list = fp_list[1]
      y_hat = fp_list[0]
      y = y_train[image_no]
      if( y_hat[y]!= 0):
        loss += loss_function(loss_fun,y_hat,y) / len(x_train)
      # validation loss
      if(image_no < len(x_val)):
        h_v = x_val[image_no].reshape(784,1)
        y_hat_v,h_list_v,a_list_v = forward_prop(activation,theta,b,num_layers,h_v)
        y_v = y_val[image_no]
        if(y_hat_v[y_v] != 0):
          val_loss += loss_function(loss_fun,y_hat_v,y_v) / len(x_val)

      # back propogation
      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation,loss_fun)
      list_grad_w=add_list_grad(list_grad_w.copy(), bp_list[0])
      list_grad_b=add_list_grad(list_grad_b.copy() , bp_list[1])
      
      if((image_no + 1) % batch_size == 0):
        list_grad_w.reverse()
        list_grad_b.reverse()
        for j in range(len(list_grad_w)):
          list_grad_w[j] += weight_decay * theta[j].transpose();
        if(len(list_m_w) == 0):
          for i in range(len(list_grad_w)):
            list_m_w.append((1 - beta1) * list_grad_w[i] )
            list_m_b.append((1 - beta1) * list_grad_b[i] )
        else :
          for i in range(len(list_grad_w)):
            list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] 
            list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] 
        
        if(len(list_v_w) == 0):
          for i in range(len(list_grad_w)):
            list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )
            list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )
        else :
          for i in range(len(list_grad_w)):
            list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2
            list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2
        
        list_mhat_w = []
        list_mhat_b = []
        list_vhat_w = []
        list_vhat_b = []
        for i in range(len(list_m_w)):
          list_mhat_w.append(list_m_w[i]/(1-beta1 ** (int(image_no /batch_size)+1)))
          list_mhat_b.append(list_m_b[i]/(1-beta1 ** (int(image_no /batch_size)+1)))
          list_vhat_w.append(list_v_w[i]/(1-beta2 ** (int(image_no /batch_size)+1)))
          list_vhat_b.append(list_v_b[i]/(1-beta2 ** (int(image_no /batch_size)+1)))

        
        for i in range(0,len(list_grad_w)):
          theta[i] -= (eta * list_mhat_w[i].transpose())/(list_vhat_w[i].transpose() + epsilon) ** 0.5
          b[i] -= (eta * list_mhat_b[i].transpose())/(list_vhat_b[i].transpose()+ epsilon) ** 0.5
        
        list_mhat_w.clear()
        list_mhat_b.clear()
        list_vhat_w.clear()
        list_vhat_b.clear()
        list_grad_w.clear()
        list_grad_b.clear()
      image_no += 1

    if (len(x_train) % batch_size != 0):
      list_grad_w.reverse()
      list_grad_b.reverse()
      for j in range(len(list_grad_w)):
          list_grad_w[j] += weight_decay * theta[j].transpose();
      if(len(list_m_w) == 0):
        for i in range(len(list_grad_w)):
          list_m_w.append((1 - beta1) * list_grad_w[i] )
          list_m_b.append((1 - beta1) * list_grad_b[i] )
      else :
        for i in range(len(list_grad_w)):
          list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] 
          list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] 
      
      if(len(list_v_w) == 0):
        for i in range(len(list_grad_w)):
          list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )
          list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )
      else :
        for i in range(len(list_grad_w)):
          list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2
          list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2
      
      list_mhat_w = []
      list_mhat_b = []
      list_vhat_w = []
      list_vhat_b = []
      for i in range(len(list_m_w)):
        list_mhat_w.append(list_m_w[i]/(1-beta1 ** int(image_no /batch_size)))
        list_mhat_b.append(list_m_b[i]/(1-beta1 ** int(image_no /batch_size)))
        list_vhat_w.append(list_v_w[i]/(1-beta2 ** int(image_no /batch_size)))
        list_vhat_b.append(list_v_b[i]/(1-beta2 ** int(image_no /batch_size)))

      nadam_term_w = []
      nadam_term_b = []
      for i in range(len(list_grad_w)):
        nadam_term_w.append( beta1 * list_mhat_w[i] + ( (1 - beta1) * list_grad_w[i]) / (1 - beta1 ** int(image_no /batch_size) ))
        nadam_term_b.append( beta1 * list_mhat_b[i] + ( (1 - beta1) * list_grad_b[i]) / (1 - beta1 ** int(image_no /batch_size) ))
      # print(theta)
      for i in range(0,len(list_grad_w)):
        theta[i] -= (eta * nadam_term_w[i].transpose())/(list_vhat_w[i].transpose() + epsilon) ** 0.5
        b[i] -= (eta * nadam_term_b[i].transpose())/(list_vhat_b[i].transpose()+ epsilon) ** 0.5
      
      nadam_term_w.clear()
      nadam_term_b.clear()
      list_mhat_w.clear()
      list_mhat_b.clear()
      list_vhat_w.clear()
      list_vhat_b.clear()
      list_grad_w.clear()
      list_grad_b.clear()
    for i in range(len(theta)):
        temp = (weight_decay / 2) * np.sum(np.square(theta[i]))
        loss += temp / len(x_train)
        val_loss += temp / len(x_val)
    print("training loss = ",loss)
    print("validation loss = ",val_loss)
    train_accuracy = image_pridiction(x_train,y_train,theta,b,num_layers,activation)
    print("trianing accuracy = ",train_accuracy)
    val_accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)
    print("validation_accuracy = ",val_accuracy)
    wandb.log({"train_accuracy": train_accuracy, "validation_accuracy": val_accuracy, "training_loss": loss, "validation cost": val_loss, 'epoch': t})
  return(theta,b)



def neural_network(x_train,y_train,x_test,y_test,eta,beta_mom,beta,beta1,beta2,epsilon,activation,initializer,optimizer,batch_size,epochs,loss_fun,weight_decay,num_layers,neurons):
  neurons = [neurons]*(num_layers+2)
  neurons[0] = 784
  neurons[-1] = 10
  if(initializer == "Xavier"):
    theta = initilize_theta_xavier(num_layers,neurons)
    b = initilize_bias(num_layers,neurons)
  elif (initializer == "random"):
    theta = initilize_theta_random_normal(num_layers,neurons)
    b = initilize_bias(num_layers,neurons)
  elif (initializer == "random_uniform"):
    theta = initilize_theta_random_uniform(num_layers,neurons)
    b = initilize_bias(num_layers,neurons)
  else :
    print("Invalid Intilizer")
  
  # invoking training fun
  if(optimizer == "sgd"):
    theta,b = SGD(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)
  elif(optimizer == "momentum"):
    theta,b = momentum_SGD(theta,b,epochs,eta,beta_mom,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)
  elif(optimizer == "rmsprop"):
    theta,b = RMS_SGD(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay,epsilon)
  elif(optimizer == "adam"):
    theta,b = adam_SGD(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay,epsilon)
  elif(optimizer == "nag"):
    theta,b = NAG(theta,b,epochs,eta,beta_mom,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay)
  elif(optimizer == "nadam"):
    theta,b = nadam(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun,weight_decay,epsilon)
  else :
    print("Invalid Optimizer")

  # testing accuracy
  accuracy = image_pridiction(x_test, y_test, theta, b, num_layers,activation)
  print("test accuracy = ",accuracy)
  return(theta,b)



def run_NN():

    wandb.init()
    config = wandb.config

    num_neurons = int(argparse.hidden_size)
    num_hidden = int(argparse.num_layers)
    init_mode = argparse.weight_init
    epochs = int(argparse.epochs)
    batch_size = int(argparse.batch_size)
    learning_rate = float(argparse.learning_rate)
    activation_f = argparse.activation
    L2_lamb = float(argparse.weight_decay)
    optimizer = argparse.optimizer
    loss_fun = argparse.loss
    beta_mom = float(argparse.momentum)
    beta = float(argparse.beta)
    beta1 = float(argparse.beta1)
    beta2 = float(argparse.beta2)
    epsilon = float(argparse.epsilon)
    weight_decay = float(argparse.weight_decay)

    # Display the hyperparameters
    run_name = "lr_{}_ac_{}_in_{}_op_{}_bs_{}_L2_{}_ep_{}_nn_{}_nh_{}".format(learning_rate, activation_f, init_mode, optimizer, batch_size, L2_lamb, epochs, num_neurons, num_hidden)
    print(run_name)
    theta , b = neural_network(x_train,y_train,x_test,y_test,learning_rate,beta_mom,beta,beta1,beta2,epsilon,activation_f,init_mode,optimizer,batch_size,epochs,loss_fun,weight_decay,num_hidden,num_neurons)
    # testing accuracy
    accuracy = image_pridiction(x_test, y_test, theta, b, num_layers,activation)
    print("test accuracy = ",accuracy)
    wandb.run.name = run_name
    wandb.run.save()

run_NN()
