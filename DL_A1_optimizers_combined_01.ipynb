{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "poDfvogCn0d-"
      },
      "outputs": [],
      "source": [
        "# Name - Raj Mahajan\n",
        "# Roll Number - CS22M067\n",
        "# Assignment 1\n",
        "# Question 3\n",
        "# Optimizers combined\n",
        "# splittin X_train into x_train and x_val using sklearn \n",
        "# calculating accuracy on validation set at each epoch\n",
        "# done"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "xRTp4NxgpbsO"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Fashion-MNIST dataset\n",
        "(X_train, Y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "X_train=X_train.reshape(X_train.shape[0],784)\n",
        "x_test=x_test.reshape(x_test.shape[0],784)\n",
        "Y_train=Y_train.reshape(60000,1)\n",
        "y_test=y_test.reshape(10000,1)\n",
        "# X_train.shape\n",
        "Y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBFE0h1Hp-k0",
        "outputId": "738453ff-ae01-49aa-eb3d-97518b759898"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the data\n",
        "X_train=X_train/255.0\n",
        "x_test=x_test/255.0\n",
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgZvKQQ4qDJ-",
        "outputId": "39ce095c-f713-4b0a-97c5-00abe019c6a0"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_val,y_train,y_val = train_test_split(X_train , Y_train , test_size = 0.1 , random_state = 100)\n",
        "x_train.shape"
      ],
      "metadata": {
        "id": "oPQgAvW_gO_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e0b304-f6e6-43f6-a7f6-be298faf6277"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(54000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def actual_prob_dist(y) :\n",
        "  ans=np.zeros((10, 1))\n",
        "  ans[y]=1\n",
        "  return(ans)\n",
        "\n",
        "def sigmoid(a) :\n",
        "  # s=1/(1+np.exp(a))\n",
        "\n",
        "  if isinstance(a,np.ndarray):\n",
        "    return 1.0 / (1.0 + np.exp(-a))\n",
        "  else :\n",
        "    if (x>=0):\n",
        "      return 1.0 / (1.0 + np.exp(-a))\n",
        "    else :\n",
        "      return np.exp(a) / ( 1.0 + np.exp(a))\n",
        "  # return(s)\n",
        "\n",
        "\n",
        "\n",
        "def derivative_sigmoid(x):\n",
        "  ans = sigmoid(x) * (1 -sigmoid(x))\n",
        "  return(ans)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def derivative_relu(x):\n",
        "    return 1*(x>0)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def derivative_tanh(x):\n",
        "    return (1 - (np.tanh(x)**2))\n",
        "\n",
        "def softmax(v):\n",
        "  # ans = np.exp(v) / np.sum(np.exp(v), axis=0)\n",
        "  z = v - max(v)\n",
        "  numerator = np.exp(z)\n",
        "  denominator = np.sum(numerator)\n",
        "  ans = numerator/denominator\n",
        "  return(ans)\n",
        "\n",
        "\n",
        "def derivative_softmax(x):\n",
        "  ans= softmax(x) * (1-softmax(x))\n",
        "  return(ans)\n",
        "\n",
        "\n",
        "def cross_entropy(y, yhat, batch_size):\n",
        "   loss = (-1.0 * np.sum(np.multiply(y, np.log(yhat))))/batch_size\n",
        "   return(loss)"
      ],
      "metadata": {
        "id": "EAuDaw6bo_Bu"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_list_grad(list_grad_x , list_grad_y):\n",
        "  if(len(list_grad_x) == 0 ):\n",
        "    return(list_grad_y)\n",
        "  for i in range(0,len(list_grad_x)):\n",
        "    list_grad_x[i] += list_grad_y[i]\n",
        "  return(list_grad_x)\n",
        "\n",
        "def add_list_grad_eta(list_grad_x , list_grad_y, eta):\n",
        "  if(len(list_grad_x) == 0 ):\n",
        "    return(list_grad_y)\n",
        "  for i in range(0,len(list_grad_x)):\n",
        "    list_grad_x[i] -= eta * list_grad_y[i].transpose()\n",
        "  return(list_grad_x)\n",
        "\n",
        "def initilize_theta_random_normal(num_layers,neurons):\n",
        "  theta=[]\n",
        "  i=0\n",
        "  temp=np.random.randn(neurons[0], neurons[1])\n",
        "  theta.append(temp)\n",
        "  while i< len(neurons)-3 :\n",
        "    temp=np.random.randn(neurons[i+1], neurons[i+2])\n",
        "    theta.append(temp)\n",
        "    i+=1\n",
        "  temp=np.random.randn(neurons[-2], neurons[-1])\n",
        "  theta.append(temp)\n",
        "  return(theta)\n",
        "\n",
        "def initilize_theta_random_uniform(num_layers,neurons):\n",
        "  theta=[]\n",
        "  i=0\n",
        "  temp=np.random.rand(neurons[0], neurons[1])\n",
        "  theta.append(temp)\n",
        "  while i< len(neurons)-3 :\n",
        "    temp=np.random.rand(neurons[i+1], neurons[i+2])\n",
        "    theta.append(temp)\n",
        "    i+=1\n",
        "  temp=np.random.rand(neurons[-2], neurons[-1])\n",
        "  theta.append(temp)\n",
        "  return(theta)\n",
        "\n",
        "def initilize_theta_xavier(num_layers,neurons):\n",
        "  theta=[]\n",
        "  i=0\n",
        "  temp=np.random.randn(neurons[0], neurons[1]) * np.sqrt(2/(neurons[0]  + neurons[1])) # xavier\n",
        "  theta.append(temp)\n",
        "  while i< len(neurons)-3 :\n",
        "    temp=np.random.randn(neurons[i+1], neurons[i+2]) * np.sqrt(2/(neurons[i+1] + neurons[i+2])) # xavier\n",
        "    theta.append(temp)\n",
        "    i+=1\n",
        "  temp=np.random.randn(neurons[-2], neurons[-1]) * np.sqrt(2/(neurons[-2] + neurons[-1])) # xavier\n",
        "  theta.append(temp)\n",
        "  return(theta)\n",
        "\n",
        "def initilize_bias(num_layers,neurons):\n",
        "  b=[]\n",
        "  i=0\n",
        "  while i < num_layers :\n",
        "    temp=np.zeros((1,neurons[i+1]))\n",
        "    b.append(temp)\n",
        "    i+=1\n",
        "  temp=np.zeros((1, neurons[-1]))\n",
        "  b.append(temp)\n",
        "  return(b)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wQc_Udj0o_DW"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def activation_fun(activation,a):\n",
        "  if(activation == \"sigmoid\"):\n",
        "    return(sigmoid(a))\n",
        "  elif(activation == \"relu\"):\n",
        "    return(relu(a))\n",
        "  elif(activation == \"tanh\"):\n",
        "    return(tanh(a))\n",
        "\n",
        "def derivative_activation_fun(activation,a):\n",
        "  if(activation == \"sigmoid\"):\n",
        "    return(derivative_sigmoid(a))\n",
        "  elif(activation == \"relu\"):\n",
        "    return(derivative_relu(a))\n",
        "  elif(activation == \"tanh\"):\n",
        "    return(derivative_tanh(a))\n",
        "  \n",
        "def loss_function(loss_fun,yhat,y):\n",
        "  if(loss_fun == \"cross_entropy\"):\n",
        "    loss = (-math.log(yhat[y]))\n",
        "  elif(loss_fun == \"mse\"):\n",
        "    Y = actual_prob_dist(y)\n",
        "    loss = (1/2) * np.sum((Y-yhat)**2)\n",
        "  return(loss)\n"
      ],
      "metadata": {
        "id": "UJCVfydl8XrV"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constructed only for 1 image \n",
        "# h will be image input if 1d vector of length 28x28\n",
        "def forward_prop(activation,theta,b,num_layers,h):\n",
        "  h_list=[]\n",
        "  h_list.append(h)\n",
        "  a_list=[]\n",
        "  for k in range(0,num_layers):\n",
        "    a = b[k].transpose() + np.matmul(theta[k].transpose() , h)\n",
        "    a_list.append(a)\n",
        "    h= activation_fun(activation,a)\n",
        "    h_list.append(h)\n",
        "  # For last output layer\n",
        "  a = b[num_layers].transpose() + np.matmul(theta[num_layers].transpose() , h)\n",
        "  a_list.append(a)\n",
        "  yhat = softmax(a)\n",
        "  return(yhat,h_list,a_list)\n"
      ],
      "metadata": {
        "id": "rzS5d-JFo_E4"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only for 1 image\n",
        "def backward_prop(theta,h_list,a_list,y,yhat,num_layers,batch_size,activation):\n",
        "  grad_a=-(actual_prob_dist(y) - yhat)\n",
        "  list_grad_w=[]\n",
        "  list_grad_b=[]\n",
        "  for k in range(num_layers , -1,-1) :\n",
        "    grad_w = np.matmul(grad_a , h_list[k].transpose()) \n",
        "    list_grad_w.append(grad_w / batch_size)\n",
        "    grad_b = grad_a.copy()\n",
        "    list_grad_b.append(grad_b / batch_size)\n",
        "    if k >0:\n",
        "      grad_h = np.matmul(theta[k] , grad_a)\n",
        "      grad_a = grad_h * derivative_activation_fun(activation,a_list[k-1])\n",
        "  return(list_grad_w , list_grad_b)\n",
        "  \n",
        "\n",
        "# grad_b.shape = 32 x 1\n",
        "# grad_w.shape = 32 x 32\n",
        "# grad_a.shape = 10/32 x 1"
      ],
      "metadata": {
        "id": "b7FjUsDzo_Gc"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_pridiction(x_test, y_test, theta, b, num_layers,activation):\n",
        "  img_no = 0\n",
        "  wrong_count = 0\n",
        "  while(img_no<len(x_test)):\n",
        "    img = x_test[img_no].reshape(784,1)\n",
        "    clas = y_test[img_no]\n",
        "    temp_ans = forward_prop(activation,theta,b,num_layers,img)\n",
        "    yhat = temp_ans[0]\n",
        "    temp = -1\n",
        "    predicted_class = -1\n",
        "    # print(yhat.shape)\n",
        "    sum=0\n",
        "    for i in range(0 , 10):\n",
        "      sum+=yhat[i]\n",
        "      if(temp < yhat[i]):\n",
        "        predicted_class = i\n",
        "        temp = yhat[i]\n",
        "    # print(sum)\n",
        "    # print(predicted_class)\n",
        "    if(clas != predicted_class):\n",
        "      wrong_count+=1\n",
        "    img_no+=1\n",
        "  accuracy = (len(x_test) - wrong_count) * 100 / len(x_test)\n",
        "  return(accuracy)"
      ],
      "metadata": {
        "id": "ugPI1ENFuZIi"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SGD(theta,b,epochs,eta,beta,actiavtion,x_train,y_train,neurons, num_layers,batch_size,loss_fun):\n",
        "  t=0\n",
        "  while t<epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "         \n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      y_hat,h_list,a_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # backward propogation\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "        \n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers,batch_size,activation)\n",
        "      list_grad_w = add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        theta = add_list_grad_eta(theta.copy() , list_grad_w, eta)\n",
        "        b = add_list_grad_eta(b.copy(), list_grad_b , eta)\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      # print(theta)\n",
        "      image_no += 1\n",
        "\n",
        "    # Update theta for last few images\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      theta = add_list_grad_eta(theta.copy() , list_grad_w, eta)\n",
        "      b = add_list_grad_eta(b.copy(), list_grad_b , eta)\n",
        "    print(loss)\n",
        "    global x_val\n",
        "    global y_val\n",
        "    accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(accuracy)\n",
        "  return(theta,b)"
      ],
      "metadata": {
        "id": "e3Jyo0kzr1jR"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_SGD(theta,b,epochs,eta,beta,actiavtion,x_train,y_train,neurons, num_layers,batch_size,loss_fun):\n",
        "\n",
        "  t=0\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    prev_grad_uw = [] # storing previous point\n",
        "    prev_grad_ub = [] # storing previous point\n",
        "    loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers,batch_size,activation)\n",
        "      list_grad_w = add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "\n",
        "      if((image_no+1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        \n",
        "        if(len(prev_grad_uw) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            prev_grad_uw.append(beta * list_grad_w[i])\n",
        "            prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "          list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "        \n",
        "        theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "        b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "        \n",
        "        prev_grad_uw = list_u_grad_w.copy()\n",
        "        prev_grad_ub = list_u_grad_b.copy()\n",
        "        list_u_grad_w.clear()\n",
        "        list_u_grad_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    # Update theta for last few images\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      \n",
        "      if(len(prev_grad_uw) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          prev_grad_uw.append(beta * list_grad_w[i])\n",
        "          prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "      for i in range(len(list_grad_w)):\n",
        "        list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "        list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "      \n",
        "      theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "      b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "      \n",
        "      prev_grad_uw = list_u_grad_w.copy()\n",
        "      prev_grad_ub = list_u_grad_b.copy()\n",
        "      list_u_grad_w.clear()\n",
        "      list_u_grad_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "\n",
        "    print(loss)\n",
        "    global x_val\n",
        "    global y_val\n",
        "    accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(accuracy)\n",
        "  return(theta,b)"
      ],
      "metadata": {
        "id": "wyITICU2r1UN"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RMS_SGD(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun):\n",
        " \n",
        "  t=0\n",
        "  # intilize theta\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    prev_grad_uw = [] # storing previous point\n",
        "    prev_grad_ub = [] # storing previous point\n",
        "    loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train) \n",
        "\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation)\n",
        "      list_grad_w=add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b=add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "\n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        \n",
        "        if(len(prev_grad_uw) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w.append((1 - beta) * list_grad_w[i] ** 2)\n",
        "            list_u_grad_b.append((1 - beta) * list_grad_b[i] ** 2)\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i] ** 2)\n",
        "            list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i] ** 2)\n",
        "        \n",
        "        for i in range(0,len(list_grad_w)):\n",
        "          theta[i] -= (eta * list_grad_w[i].transpose())/(list_u_grad_w[i].transpose() + 1e-6) ** 0.5\n",
        "          b[i] -= (eta * list_grad_b[i].transpose())/(list_u_grad_b[i].transpose()+ 1e-6) ** 0.5\n",
        "        \n",
        "        prev_grad_uw = list_u_grad_w.copy()\n",
        "        prev_grad_ub = list_u_grad_b.copy()\n",
        "        list_u_grad_w.clear()\n",
        "        list_u_grad_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      \n",
        "      if(len(prev_grad_uw) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          prev_grad_uw.append(beta * list_grad_w[i])\n",
        "          prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "      for i in range(len(list_grad_w)):\n",
        "        list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i] ** 2)\n",
        "        list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i] ** 2)\n",
        "      \n",
        "      for i in range(0,len(list_grad_w)):\n",
        "        theta[i] -= (eta * list_grad_w[i].transpose())/(list_u_grad_w[i].transpose() + 1e-6) ** 0.5\n",
        "        b[i] -= (eta * list_grad_b[i].transpose())/(list_u_grad_b[i].transpose()+ 1e-6) ** 0.5\n",
        "      \n",
        "      prev_grad_uw = list_u_grad_w.copy()\n",
        "      prev_grad_ub = list_u_grad_b.copy()\n",
        "      list_u_grad_w.clear()\n",
        "      list_u_grad_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "    print(loss)\n",
        "    global x_val\n",
        "    global y_val\n",
        "    accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(accuracy)\n",
        "  return(theta,b)\n"
      ],
      "metadata": {
        "id": "N7Gz1W23EO7U"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adam_SGD(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun):\n",
        "  \n",
        "  t=0\n",
        "  # intilize theta\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    list_m_w = [] # storing previous point\n",
        "    list_m_b = [] # storing previous point\n",
        "    list_v_w = [] # storing previous point\n",
        "    list_v_b = [] # storing previous point\n",
        "    loss = 0\n",
        "    while( image_no < len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation)\n",
        "      list_grad_w=add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b=add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      \n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        \n",
        "        if(len(list_m_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "            list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "            list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "        \n",
        "        if(len(list_v_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "            list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "            list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "        \n",
        "        list_mhat_w = []\n",
        "        list_mhat_b = []\n",
        "        list_vhat_w = []\n",
        "        list_vhat_b = []\n",
        "        for i in range(len(list_m_w)):\n",
        "          list_mhat_w.append(list_m_w[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_mhat_b.append(list_m_b[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_w.append(list_v_w[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_b.append(list_v_b[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "\n",
        "        \n",
        "        # print(theta)\n",
        "        for i in range(0,len(list_grad_w)):\n",
        "          theta[i] -= (eta * list_mhat_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "          b[i] -= (eta * list_mhat_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "        \n",
        "        list_mhat_w.clear()\n",
        "        list_mhat_b.clear()\n",
        "        list_vhat_w.clear()\n",
        "        list_vhat_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      \n",
        "      if(len(list_m_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "          list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "          list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "      \n",
        "      if(len(list_v_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "          list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "          list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "      \n",
        "      list_mhat_w = []\n",
        "      list_mhat_b = []\n",
        "      list_vhat_w = []\n",
        "      list_vhat_b = []\n",
        "      for i in range(len(list_m_w)):\n",
        "        list_mhat_w.append(list_m_w[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_mhat_b.append(list_m_b[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_vhat_w.append(list_v_w[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "        list_vhat_b.append(list_v_b[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "\n",
        "      \n",
        "      # print(theta)\n",
        "      for i in range(0,len(list_grad_w)):\n",
        "        theta[i] -= (eta * list_mhat_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "        b[i] -= (eta * list_mhat_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "      \n",
        "      list_mhat_w.clear()\n",
        "      list_mhat_b.clear()\n",
        "      list_vhat_w.clear()\n",
        "      list_vhat_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "    print(loss)\n",
        "    global x_val\n",
        "    global y_val\n",
        "    accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(accuracy)\n",
        "  return(theta,b)\n"
      ],
      "metadata": {
        "id": "7d67UB_mGtGb"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NAG(theta,b,epochs,eta,beta,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun):\n",
        "  \n",
        "  t=0\n",
        "  # intilize theta\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    prev_grad_uw = [] # storing previous point\n",
        "    prev_grad_ub = [] # storing previous point\n",
        "    loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      w_lookahead = []\n",
        "      b_lookahead = []\n",
        "      if(len(prev_grad_uw)==0):\n",
        "        w_lookahead = theta.copy()\n",
        "        b_lookahead = b.copy();\n",
        "      else :\n",
        "        for i in range(len(prev_grad_uw)):\n",
        "          w_lookahead.append(theta[i] - beta * prev_grad_uw[i].transpose())\n",
        "          b_lookahead.append(b[i] - beta * prev_grad_ub[i].transpose())\n",
        "      \n",
        "      fp_list = forward_prop(activation,w_lookahead,b_lookahead,num_layers,h)\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train) \n",
        "    \n",
        "      bp_list = backward_prop(w_lookahead,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation)\n",
        "      list_grad_w = add_list_grad(list_grad_w.copy() , bp_list[0])\n",
        "      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      w_lookahead.clear()\n",
        "      b_lookahead.clear()\n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        \n",
        "        if(len(prev_grad_uw) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w = list_grad_w.copy()\n",
        "            list_u_grad_b = list_grad_b.copy()\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "            list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "        \n",
        "        # print(theta)\n",
        "        theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "        b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "        \n",
        "        prev_grad_uw = list_u_grad_w.copy()\n",
        "        prev_grad_ub = list_u_grad_b.copy()\n",
        "        list_u_grad_w.clear()\n",
        "        list_u_grad_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "        \n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      \n",
        "      if(len(prev_grad_uw) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          prev_grad_uw.append(beta * list_grad_w[i])\n",
        "          prev_grad_ub.append(beta * list_grad_b[i])\n",
        "\n",
        "      for i in range(len(list_grad_w)):\n",
        "        list_u_grad_w.append(beta * prev_grad_uw[i] + (1 - beta) * list_grad_w[i])\n",
        "        list_u_grad_b.append(beta * prev_grad_ub[i] + (1 - beta) * list_grad_b[i])\n",
        "      \n",
        "      theta = add_list_grad_eta(theta.copy() , list_u_grad_w, eta)\n",
        "      b = add_list_grad_eta(b.copy(), list_u_grad_b , eta)\n",
        "      \n",
        "      prev_grad_uw = list_u_grad_w.copy()\n",
        "      prev_grad_ub = list_u_grad_b.copy()\n",
        "      list_u_grad_w.clear()\n",
        "      list_u_grad_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "    print(loss)\n",
        "    global x_val\n",
        "    global y_val\n",
        "    accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(accuracy)\n",
        "  return(theta,b)\n"
      ],
      "metadata": {
        "id": "WZmDJQ4BfEpt"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nadam(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun):\n",
        "\n",
        "  t=0\n",
        "  # intilize theta\n",
        "  while t < epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_u_grad_w = []\n",
        "    list_u_grad_b = []\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    list_m_w = [] # storing previous point\n",
        "    list_m_b = [] # storing previous point\n",
        "    list_v_w = [] # storing previous point\n",
        "    list_v_b = [] # storing previous point\n",
        "    loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      fp_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "\n",
        "      # backward propogation\n",
        "      a_list = fp_list[2]\n",
        "      h_list = fp_list[1]\n",
        "      y_hat = fp_list[0]\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += loss_function(loss_fun,y_hat,y) / len(x_train)\n",
        "\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers ,batch_size,activation)\n",
        "      list_grad_w=add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b=add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      \n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        \n",
        "        if(len(list_m_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "            list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "            list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "        \n",
        "        if(len(list_v_w) == 0):\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "            list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "        else :\n",
        "          for i in range(len(list_grad_w)):\n",
        "            list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "            list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "        \n",
        "        list_mhat_w = []\n",
        "        list_mhat_b = []\n",
        "        list_vhat_w = []\n",
        "        list_vhat_b = []\n",
        "        for i in range(len(list_m_w)):\n",
        "          list_mhat_w.append(list_m_w[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_mhat_b.append(list_m_b[i]/(1-beta1 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_w.append(list_v_w[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "          list_vhat_b.append(list_v_b[i]/(1-beta2 ** (int(image_no /batch_size)+1)))\n",
        "\n",
        "        \n",
        "        for i in range(0,len(list_grad_w)):\n",
        "          theta[i] -= (eta * list_mhat_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "          b[i] -= (eta * list_mhat_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "        \n",
        "        list_mhat_w.clear()\n",
        "        list_mhat_b.clear()\n",
        "        list_vhat_w.clear()\n",
        "        list_vhat_b.clear()\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      image_no += 1\n",
        "\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      \n",
        "      if(len(list_m_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w.append((1 - beta1) * list_grad_w[i] )\n",
        "          list_m_b.append((1 - beta1) * list_grad_b[i] )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_m_w[i] = beta1 * list_m_w[i] + (1 - beta1) * list_grad_w[i] \n",
        "          list_m_b[i] = beta1 * list_m_b[i] + (1 - beta1) * list_grad_b[i] \n",
        "      \n",
        "      if(len(list_v_w) == 0):\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w.append((1 - beta2) * list_grad_w[i] ** 2 )\n",
        "          list_v_b.append((1 - beta2) * list_grad_b[i] ** 2 )\n",
        "      else :\n",
        "        for i in range(len(list_grad_w)):\n",
        "          list_v_w[i] = beta2 * list_v_w[i] + (1 - beta2) * list_grad_w[i] ** 2\n",
        "          list_v_b[i] = beta2 * list_v_b[i] + (1 - beta2) * list_grad_b[i] ** 2\n",
        "      \n",
        "      list_mhat_w = []\n",
        "      list_mhat_b = []\n",
        "      list_vhat_w = []\n",
        "      list_vhat_b = []\n",
        "      for i in range(len(list_m_w)):\n",
        "        list_mhat_w.append(list_m_w[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_mhat_b.append(list_m_b[i]/(1-beta1 ** int(image_no /batch_size)))\n",
        "        list_vhat_w.append(list_v_w[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "        list_vhat_b.append(list_v_b[i]/(1-beta2 ** int(image_no /batch_size)))\n",
        "\n",
        "      nadam_term_w = []\n",
        "      nadam_term_b = []\n",
        "      for i in range(len(list_grad_w)):\n",
        "        nadam_term_w.append( beta1 * list_mhat_w[i] + ( (1 - beta1) * list_grad_w[i]) / (1 - beta1 ** int(image_no /batch_size) ))\n",
        "        nadam_term_b.append( beta1 * list_mhat_b[i] + ( (1 - beta1) * list_grad_b[i]) / (1 - beta1 ** int(image_no /batch_size) ))\n",
        "      # print(theta)\n",
        "      for i in range(0,len(list_grad_w)):\n",
        "        theta[i] -= (eta * nadam_term_w[i].transpose())/(list_vhat_w[i].transpose() + 1e-6) ** 0.5\n",
        "        b[i] -= (eta * nadam_term_b[i].transpose())/(list_vhat_b[i].transpose()+ 1e-6) ** 0.5\n",
        "      \n",
        "      nadam_term_w.clear()\n",
        "      nadam_term_b.clear()\n",
        "      list_mhat_w.clear()\n",
        "      list_mhat_b.clear()\n",
        "      list_vhat_w.clear()\n",
        "      list_vhat_b.clear()\n",
        "      list_grad_w.clear()\n",
        "      list_grad_b.clear()\n",
        "    print(loss)\n",
        "    global x_val\n",
        "    global y_val\n",
        "    accuracy = image_pridiction(x_val,y_val,theta,b,num_layers,activation)\n",
        "    print(accuracy)\n",
        "  return(theta,b)\n"
      ],
      "metadata": {
        "id": "H-M3MQXIuGbo"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "ACTIVATION = \"sigmoid\"\n",
        "INITIALIZER = \"xavier\"\n",
        "OPTIMIZER = \"nag\"\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "L2_lambda = 0.0005\n",
        "LAYER_DIMS = [784,64,32,64,10]\n",
        "\n",
        "LOSS = 'categorical_crossentropy'\n",
        "\n",
        "\n",
        "learning_rate = 0.01\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "activation = \"tanh\"\n",
        "initializer = \"xavier\"\n",
        "optimizer = \"nadam\"\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "loss_fun = \"cross_entropy\"\n",
        "neurons = [784,32,32,10]\n",
        "num_layers = len(neurons) - 2"
      ],
      "metadata": {
        "id": "O1eNrF00vC9A"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network(x_train,y_train,x_test,y_test,eta,beta1,beta2,activation,initializer,optimizer,batch_size,epochs,loss_fun):\n",
        "  if(initializer == \"xavier\"):\n",
        "    theta = initilize_theta_xavier(num_layers,neurons)\n",
        "    b = initilize_bias(num_layers,neurons)\n",
        "  elif (initializer == \"random_normal\"):\n",
        "    theta = initilize_theta_random_normal(num_layers,neurons)\n",
        "    b = initilize_bias(num_layers,neurons)\n",
        "  elif (initializer == \"random_uniform\"):\n",
        "    theta = initilize_theta_random_uniform(num_layers,neurons)\n",
        "    b = initilize_bias(num_layers,neurons)\n",
        "  else :\n",
        "    print(\"Invalid Intilizer\")\n",
        "  \n",
        "  # invoking training fun\n",
        "  if(optimizer == \"sgd\"):\n",
        "    theta,b = SGD(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun)\n",
        "  elif(optimizer == \"momentum\"):\n",
        "    theta,b = momentum_SGD(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun)\n",
        "  elif(optimizer == \"rms\"):\n",
        "    theta,b = RMS_SGD(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun)\n",
        "  elif(optimizer == \"adam\"):\n",
        "    theta,b = adam_SGD(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun)\n",
        "  elif(optimizer == \"nag\"):\n",
        "    theta,b = NAG(theta,b,epochs,eta,beta1,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun)\n",
        "  elif(optimizer == \"nadam\"):\n",
        "    theta,b = nadam(theta,b,epochs,eta,beta1,beta2,activation,x_train,y_train,neurons, num_layers,batch_size,loss_fun)\n",
        "  else :\n",
        "    print(\"Invalid Optimizer\")\n",
        "\n",
        "\n",
        "  return(theta,b)\n",
        " \n"
      ],
      "metadata": {
        "id": "SWc5Sxa1o_J5"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta , b =neural_network(x_train,y_train,x_test,y_test,learning_rate,beta1,beta2,activation,initializer,optimizer,batch_size,epochs,loss_fun)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC8P9Y4RC73D",
        "outputId": "b9ea6d94-dc17-428c-94d5-0441d7c4d376"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0.6507847238949354\n",
            "78.05\n",
            "2\n",
            "0.5752294487735543\n",
            "80.5\n",
            "3\n",
            "0.565755304653903\n",
            "81.2\n",
            "4\n",
            "0.5366265333182937\n",
            "82.13333333333334\n",
            "5\n",
            "0.532896001788721\n",
            "79.6\n",
            "6\n",
            "0.5257536807396498\n",
            "83.36666666666666\n",
            "7\n",
            "0.5364594653348478\n",
            "78.6\n",
            "8\n",
            "0.536529087716721\n",
            "79.55\n",
            "9\n",
            "0.5332267637585235\n",
            "81.76666666666667\n",
            "10\n",
            "0.5292632366922758\n",
            "82.46666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # testing accuracy\n",
        "accuracy = image_pridiction(x_test, y_test, theta, b, num_layers,activation)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "ncr6h7igElO1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ec2755-b024-4c2e-d020-3687579a2373"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2npDhTfaDH7V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}