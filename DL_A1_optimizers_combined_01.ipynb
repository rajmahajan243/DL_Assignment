{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poDfvogCn0d-"
      },
      "outputs": [],
      "source": [
        "# Name - Raj Mahajan\n",
        "# Roll Number - CS22M067\n",
        "# Assignment 1\n",
        "# Question 3\n",
        "# Optimizers combined"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist"
      ],
      "metadata": {
        "id": "xRTp4NxgpbsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Fashion-MNIST dataset\n",
        "(X_train, Y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "X_train=X_train.reshape(X_train.shape[0],784)\n",
        "x_test=x_test.reshape(x_test.shape[0],784)\n",
        "Y_train=Y_train.reshape(60000,1)\n",
        "y_test=y_test.reshape(10000,1)\n",
        "# X_train.shape\n",
        "Y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBFE0h1Hp-k0",
        "outputId": "fce81be9-d3a3-4a58-bbdf-bfd2fb391ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the data\n",
        "X_train=X_train/255.0\n",
        "x_test=x_test/255.0\n",
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgZvKQQ4qDJ-",
        "outputId": "dbf9f0da-2c22-4ba1-c208-e9f642cb8410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def actual_prob_dist(y) :\n",
        "  ans=np.zeros((10, 1))\n",
        "  ans[y]=1\n",
        "  return(ans)\n",
        "\n",
        "def sigmoid(a) :\n",
        "  # s=1/(1+np.exp(a))\n",
        "\n",
        "  if isinstance(a,np.ndarray):\n",
        "    return 1.0 / (1.0 + np.exp(-a))\n",
        "  else :\n",
        "    if (x>=0):\n",
        "      return 1.0 / (1.0 + np.exp(-a))\n",
        "    else :\n",
        "      return np.exp(a) / ( 1.0 + np.exp(a))\n",
        "  # return(s)\n",
        "\n",
        "\n",
        "\n",
        "def derivative_sigmoid(x):\n",
        "  ans = sigmoid(x) * (1 -sigmoid(x))\n",
        "  return(ans)\n",
        "\n",
        "\n",
        "def softmax(v):\n",
        "  # ans = np.exp(v) / np.sum(np.exp(v), axis=0)\n",
        "  z = v - max(v)\n",
        "  numerator = np.exp(z)\n",
        "  denominator = np.sum(numerator)\n",
        "  ans = numerator/denominator\n",
        "  return(ans)\n",
        "\n",
        "\n",
        "def derivative_softmax(x):\n",
        "  ans= softmax(x) * (1-softmax(x))\n",
        "  return(ans)\n",
        "\n",
        "\n",
        "def cross_entropy(y, yhat, batch_size):\n",
        "   loss = (-1.0 * np.sum(np.multiply(y, np.log(yhat))))/batch_size\n",
        "   return(loss)"
      ],
      "metadata": {
        "id": "EAuDaw6bo_Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_list_grad(list_grad_x , list_grad_y):\n",
        "  if(len(list_grad_x) == 0 ):\n",
        "    return(list_grad_y)\n",
        "  for i in range(0,len(list_grad_x)):\n",
        "    list_grad_x[i] += list_grad_y[i]\n",
        "  return(list_grad_x)\n",
        "\n",
        "def add_list_grad_eta(list_grad_x , list_grad_y, eta):\n",
        "  if(len(list_grad_x) == 0 ):\n",
        "    return(list_grad_y)\n",
        "  for i in range(0,len(list_grad_x)):\n",
        "    list_grad_x[i] -= eta * list_grad_y[i].transpose()\n",
        "  return(list_grad_x)\n",
        "\n",
        "def initilize_theta_random(num_layers,num_neurons):\n",
        "  theta=[]\n",
        "  i=0\n",
        "  temp=np.random.randn(28 * 28, num_neurons)\n",
        "  theta.append(temp)\n",
        "  while i< num_layers-1 :\n",
        "    temp=np.random.randn(num_neurons, num_neurons)\n",
        "    theta.append(temp)\n",
        "    i+=1\n",
        "  temp=np.random.randn(num_neurons, 10) \n",
        "  theta.append(temp)\n",
        "  return(theta)\n",
        "\n",
        "def initilize_theta_xavier(num_layers,num_neurons):\n",
        "  theta=[]\n",
        "  i=0\n",
        "  temp=np.random.randn(28 * 28, num_neurons) * np.sqrt(2/(28 * 28 + num_neurons)) # xavier\n",
        "  theta.append(temp)\n",
        "  while i< num_layers-1 :\n",
        "    temp=np.random.randn(num_neurons, num_neurons) * np.sqrt(2/(num_neurons + num_neurons)) # xavier\n",
        "    theta.append(temp)\n",
        "    i+=1\n",
        "  temp=np.random.randn(num_neurons, 10) * np.sqrt(2/(num_neurons + 10)) # xavier\n",
        "  theta.append(temp)\n",
        "  return(theta)\n",
        "\n",
        "def initilize_bias(num_layers,num_neurons):\n",
        "  b=[]\n",
        "  i=0\n",
        "  while i < num_layers :\n",
        "    temp=np.zeros((1, num_neurons))\n",
        "    b.append(temp)\n",
        "    i+=1\n",
        "  temp=np.zeros((1, 10))\n",
        "  b.append(temp)\n",
        "  return(b)\n",
        "\n",
        "\n",
        "def image_pridiction(x_test, y_test, theta, b, num_layers,activation):\n",
        "  img_no = 0\n",
        "  wrong_count = 0\n",
        "  while(img_no<len(x_test)):\n",
        "    img = x_test[img_no].reshape(784,1)\n",
        "    clas = y_test[img_no]\n",
        "    temp_ans = forward_prop(activation,theta,b,num_layers,img)\n",
        "    yhat = temp_ans[0]\n",
        "    temp = -1\n",
        "    predicted_class = -1\n",
        "    # print(yhat.shape)\n",
        "    sum=0\n",
        "    for i in range(0 , 10):\n",
        "      sum+=yhat[i]\n",
        "      if(temp < yhat[i]):\n",
        "        predicted_class = i\n",
        "        temp = yhat[i]\n",
        "    # print(sum)\n",
        "    # print(predicted_class)\n",
        "    if(clas != predicted_class):\n",
        "      wrong_count+=1\n",
        "    img_no+=1\n",
        "  return(wrong_count)"
      ],
      "metadata": {
        "id": "wQc_Udj0o_DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def activation_fun(activation,a):\n",
        "  if(activation == \"sigmoid\"):\n",
        "    return(sigmoid(a))\n",
        "  "
      ],
      "metadata": {
        "id": "UJCVfydl8XrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constructed only for 1 image \n",
        "# h will be image input if 1d vector of length 28x28\n",
        "def forward_prop(activation,theta,b,num_layers,h):\n",
        "  h_list=[]\n",
        "  h_list.append(h)\n",
        "  a_list=[]\n",
        "  for k in range(0,num_layers):\n",
        "    a = b[k].transpose() + np.matmul(theta[k].transpose() , h)\n",
        "    a_list.append(a)\n",
        "    h= activation_fun(activation,a)\n",
        "    h_list.append(h)\n",
        "  # For last output layer\n",
        "  a = b[num_layers].transpose() + np.matmul(theta[num_layers].transpose() , h)\n",
        "  a_list.append(a)\n",
        "  yhat = softmax(a)\n",
        "  return(yhat,h_list,a_list)\n"
      ],
      "metadata": {
        "id": "rzS5d-JFo_E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only for 1 image\n",
        "def backward_prop(theta,h_list,a_list,y,yhat,num_layers):\n",
        "  grad_a=-(actual_prob_dist(y) - yhat)\n",
        "  list_grad_w=[]\n",
        "  list_grad_b=[]\n",
        "  for k in range(num_layers , -1,-1) :\n",
        "    grad_w = np.matmul(grad_a , h_list[k].transpose())\n",
        "    list_grad_w.append(grad_w)\n",
        "    grad_b = grad_a.copy()\n",
        "    list_grad_b.append(grad_b)\n",
        "    if k >0:\n",
        "      grad_h = np.matmul(theta[k] , grad_a)\n",
        "      grad_a = grad_h * derivative_sigmoid(a_list[k-1])\n",
        "  return(list_grad_w , list_grad_b)\n",
        "  \n",
        "\n",
        "# grad_b.shape = 32 x 1\n",
        "# grad_w.shape = 32 x 32\n",
        "# grad_a.shape = 10/32 x 1"
      ],
      "metadata": {
        "id": "b7FjUsDzo_Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ugPI1ENFuZIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SGD(theta,b,epochs,eta,actiavtion,x_train,y_train,num_neurons, num_layers,batch_size):\n",
        "  t=0\n",
        "  while t<epochs :\n",
        "    t += 1\n",
        "    print(t)\n",
        "    image_no = 0\n",
        "    list_grad_w = []\n",
        "    list_grad_b = []\n",
        "    loss = 0\n",
        "    while(image_no<len(x_train)):\n",
        "         \n",
        "      # forward propgation\n",
        "      h=x_train[image_no].reshape(784,1)\n",
        "      y_hat,h_list,a_list = forward_prop(activation,theta,b,num_layers,h)\n",
        "      # backward propogation\n",
        "      y = y_train[image_no]\n",
        "      if( y_hat[y]!= 0):\n",
        "        loss += (-math.log(y_hat[y]))\n",
        "      bp_list = backward_prop(theta,h_list,a_list,y_train[image_no],y_hat,num_layers)\n",
        "      list_grad_w = add_list_grad(list_grad_w.copy(), bp_list[0])\n",
        "      list_grad_b = add_list_grad(list_grad_b.copy() , bp_list[1])\n",
        "      if((image_no + 1) % batch_size == 0):\n",
        "        list_grad_w.reverse()\n",
        "        list_grad_b.reverse()\n",
        "        theta = add_list_grad_eta(theta.copy() , list_grad_w, eta)\n",
        "        b = add_list_grad_eta(b.copy(), list_grad_b , eta)\n",
        "        list_grad_w.clear()\n",
        "        list_grad_b.clear()\n",
        "      # print(theta)\n",
        "      image_no += 1\n",
        "\n",
        "    # Update theta for last few images\n",
        "    if (len(x_train) % batch_size != 0):\n",
        "      list_grad_w.reverse()\n",
        "      list_grad_b.reverse()\n",
        "      theta = add_list_grad_eta(theta.copy() , list_grad_w, eta)\n",
        "      b = add_list_grad_eta(b.copy(), list_grad_b , eta)\n",
        "    print(loss/len(x_train))\n",
        "    # list_grad_w.reverse()\n",
        "    # list_grad_b.reverse()\n",
        "    # theta = add_list_grad_eta(theta , list_grad_w, eta)\n",
        "    # b = add_list_grad_eta(b, list_grad_b , eta)\n",
        "  return(theta,b)"
      ],
      "metadata": {
        "id": "e3Jyo0kzr1jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_NN():\n"
      ],
      "metadata": {
        "id": "wyITICU2r1UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "ACTIVATION = \"sigmoid\"\n",
        "INITIALIZER = \"xavier\"\n",
        "OPTIMIZER = \"sgd\"\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 40\n",
        "L2_lambda = 0.0005\n",
        "LAYER_DIMS = [784,64,32,64,10]\n",
        "\n",
        "LOSS = 'categorical_crossentropy'\n",
        "\n",
        "\n",
        "learning_rate = 0.01\n",
        "activation = \"sigmoid\"\n",
        "initializer = \"xavier\"\n",
        "optimizer = \"sgd\"\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "loss_fun = \"cross_entropy\"\n",
        "num_layers = 2\n",
        "num_neurons = 32"
      ],
      "metadata": {
        "id": "O1eNrF00vC9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network(x_train,y_train,x_test,y_test,eta,activation,initializer,optimizer,batch_size,epochs,loss_fun):\n",
        "  if(initializer == \"xavier\"):\n",
        "    theta = initilize_theta_xavier(num_layers,num_neurons)\n",
        "    b = initilize_bias(num_layers,num_neurons)\n",
        "  elif (initializer == \"random\"):\n",
        "    theta = initilize_theta_random(num_layers,num_neurons)\n",
        "    b = initilize_bias(num_layers,num_neurons)\n",
        "  \n",
        "  # invoking training fun\n",
        "  if(optimizer == \"sgd\"):\n",
        "    theta,b = SGD(theta,b,epochs,eta,activation,x_train,y_train,num_neurons, num_layers,batch_size)\n",
        "\n",
        "\n",
        "  return(theta,b)\n",
        " \n"
      ],
      "metadata": {
        "id": "SWc5Sxa1o_J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta , b =neural_network(X_train,Y_train,x_test,y_test,learning_rate,activation,initializer,optimizer,batch_size,epochs,loss_fun)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC8P9Y4RC73D",
        "outputId": "5d53562d-8372-4c02-fc22-79f08dff92ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0.7321355820127212\n",
            "2\n",
            "0.446545657506687\n",
            "3\n",
            "0.4010977821052966\n",
            "4\n",
            "0.37478364311363876\n",
            "5\n",
            "0.35628150932632796\n",
            "6\n",
            "0.34201627765902176\n",
            "7\n",
            "0.33051204749600577\n",
            "8\n",
            "0.32082530639015655\n",
            "9\n",
            "0.3123108727884669\n",
            "10\n",
            "0.30472041354605384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # testing accuracy\n",
        "wrong_count=image_pridiction(x_test, y_test, theta, b, num_layers,activation)\n",
        "print(wrong_count)\n",
        "accuracy = (len(x_test) - wrong_count) * 100 / len(x_test)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncr6h7igElO1",
        "outputId": "94b76370-4147-4fb0-82c9-e573754b9a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1313\n",
            "86.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2npDhTfaDH7V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}